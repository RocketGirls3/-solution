{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from collections import Counter\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Input, Embedding, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import optimizers, losses, activations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from keras.layers.noise import GaussianNoise\n",
    "# from models_def import Attention\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "config = argparse.Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =\"weights_base.best.h5\"\n",
    "config.len_desc = 800000\n",
    "config.maxlen = 1000\n",
    "max_features = 800000\n",
    "vec_len = 600\n",
    "count_thres = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "der=False\n",
    "if der:\n",
    "    train_dir = '../input/new_data/train_set.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "else:\n",
    "#     train_dir = '../input/new_data/train_remove60.csv'\n",
    "#     test_dir = '../input/new_data/test_remove60.csv'\n",
    "    train_dir = '../input/new_data/shuffle_train.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "\n",
    "train = pd.read_csv(train_dir)\n",
    "test = pd.read_csv(test_dir)\n",
    "original_train = pd.read_csv('../input/new_data/train_set.csv')\n",
    "shuffle_data = train[original_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "train_offset = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "df_y_train = (train[\"class\"]-1).astype(int)\n",
    "# column = \"word_seg\"\n",
    "test_id = test[[\"id\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"word_seg\"\n",
    "\n",
    "tknzr_word = Tokenizer(num_words=config.len_desc)\n",
    "tknzr_word.fit_on_texts(all_data[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 897523/897523 [00:01<00:00, 755498.65it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "low_count_words = [w for w, c in tknzr_word.word_counts.items() if c < count_thres]\n",
    "# print(len(tknzr_word.texts_to_sequences(all_data[column].values)))\n",
    "for w in tqdm(low_count_words):\n",
    "    del tknzr_word.word_index[w]\n",
    "    del tknzr_word.word_docs[w]\n",
    "    del tknzr_word.word_counts[w]\n",
    "# print(len(tknzr_word.texts_to_sequences(all_data[column].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_word_seq = tknzr_word.texts_to_sequences(train[column].values)\n",
    "te_word_seq = tknzr_word.texts_to_sequences(test[column].values)\n",
    "\n",
    "tr_word_pad = pad_sequences(tr_word_seq, maxlen=config.maxlen)\n",
    "te_word_pad = pad_sequences(te_word_seq, maxlen=config.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "EMBEDDING = '../feature/word2vec_file/avito600d.w2v'\n",
    "model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "word_index = tknzr_word.word_index\n",
    "nb_words_desc = min(max_features, len(word_index))\n",
    "embedding_matrix_desc = np.zeros((nb_words_desc+1, vec_len))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix_desc[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_y_train=np_utils.to_categorical(Y,num_classes=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvlist2 = list(StratifiedKFold(n_splits=5, random_state=786).split(tr_word_pad, Y))\n",
    "cvlist2 = list(StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=786).split(tr_word_pad, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        # self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        # print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class GRUClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, gru_dim=150, dense_dim=256, batch_size=128, epochs=2, bidirectional=False, \n",
    "                 pool_type='all', initial_weights=None, optimizer='adam' ,verbose=1, out_dim=19, callbacks=None,\n",
    "                spatial_drop=0.0, dropout=0.0, mask_zero=True, \n",
    "                gru_kernel_regularization = 0.0001,\n",
    "                gru_recurrent_regularization = 0.0001,\n",
    "                gru_bias_regularization = 0.0001,\n",
    "                embeddings_regularization = 0.0,\n",
    "                ):\n",
    "        \n",
    "        self.gru_dim = gru_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs= epochs\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pool_type = pool_type\n",
    "        self.initial_weights = initial_weights\n",
    "        self.verbose = verbose\n",
    "        self.callbacks = callbacks\n",
    "        self.optimizer = optimizer\n",
    "        self.out_dim = out_dim\n",
    "        self.spatial_drop = spatial_drop\n",
    "        self.dropout = dropout\n",
    "        self.mask_zero = mask_zero\n",
    "        self.gru_kernel_regularization = gru_kernel_regularization\n",
    "        self.gru_recurrent_regularization = gru_recurrent_regularization\n",
    "        self.gru_bias_regularization = gru_bias_regularization\n",
    "        self.embeddings_regularization = embeddings_regularization\n",
    "        \n",
    "    def _build_model(self):\n",
    "        inp = Input(shape=(config.maxlen,))\n",
    "        emb = Embedding(self.initial_weights.shape[0], \n",
    "                        600,\n",
    "                        weights=[self.initial_weights],\n",
    "                        mask_zero=self.mask_zero,\n",
    "                        #embeddings_regularizer=regularizers.l2(self.embeddings_regularization),\n",
    "                        trainable=False)(inp)\n",
    "\n",
    "        if self.mask_zero:\n",
    "            emb = ZeroMaskedLayer()(emb)\n",
    "            \n",
    "        emb = SpatialDropout1D(self.spatial_drop)(emb)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            enc = Bidirectional(CuDNNLSTM(int(self.gru_dim), return_sequences=True, return_state=True,\n",
    "                                         ))(emb)\n",
    "            x = enc[0]\n",
    "            state = enc[1]\n",
    "            enc = Bidirectional(CuDNNGRU(int(self.gru_dim), return_sequences=True, return_state=True,\n",
    "                                         ))(x)\n",
    "            x = enc[0]\n",
    "            state = enc[1]\n",
    "        else:\n",
    "            x, state = CuDNNLSTM(int(self.gru_dim), return_sequences=True, return_state=True,\n",
    "                            kernel_regularizer=regularizers.l2(self.gru_kernel_regularization),\n",
    "                            recurrent_regularizer=regularizers.l2(self.gru_recurrent_regularization),\n",
    "                            bias_regularizer=regularizers.l2(self.gru_bias_regularization)\n",
    "                               )(emb)\n",
    "            x, state = CuDNNGRU(int(self.gru_dim), return_sequences=True, return_state=True,\n",
    "                kernel_regularizer=regularizers.l2(self.gru_kernel_regularization),\n",
    "                recurrent_regularizer=regularizers.l2(self.gru_recurrent_regularization),\n",
    "                bias_regularizer=regularizers.l2(self.gru_bias_regularization)\n",
    "                   )(x)\n",
    "\n",
    "        \n",
    "        if self.pool_type == 'avg':\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'max':\n",
    "            x = GlobalMaxPool1D()(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'attn':\n",
    "            x = AttentionLayer(config.maxlen)(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'attn_avg':\n",
    "            x = AttentionWeightedAverage()(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'all':\n",
    "            #x1 = GlobalAveragePooling1D()(emb)\n",
    "            x2 = GlobalMaxPool1D()(x)\n",
    "            x3 = AttentionLayer(config.maxlen)(x)\n",
    "            x4 = Dot(1)([x2, x3])\n",
    "            x = concatenate([x2, x3, x4])\n",
    "    \n",
    "        x = Dropout(self.dropout)(x)\n",
    "        x = Dense(self.dense_dim)(x)\n",
    "        x = PReLU()(x)\n",
    "        \n",
    "        #x = Dropout(self.dropout)(x)\n",
    "        #x = Dense(256)(x)\n",
    "        #x = PReLU()(x)\n",
    "\n",
    "        out = Dense(self.out_dim, activation=\"softmax\")(x)\n",
    "        if self.optimizer == 'adam':\n",
    "            opt = Adam(lr=0.001, decay=0.0,)\n",
    "        if self.optimizer == 'nadam':\n",
    "            opt = Nadam(lr=0.002)\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            opt = RMSprop(clipnorm=1.0)\n",
    "        model = Model(inputs=inp, outputs=out)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[f1])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, hold_out_x1, hold_out_y):\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        if self.callbacks:\n",
    "            self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose,\n",
    "                       callbacks=self.callbacks,\n",
    "                       validation_data=(hold_out_x1, hold_out_y),\n",
    "                       shuffle=True)\n",
    "        else:\n",
    "            self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose,\n",
    "                        validation_data=(hold_out_x1, hold_out_y),\n",
    "                       shuffle=True)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        if self.model:\n",
    "            y_hat = self.model.predict(X, batch_size=1024)\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list = [{'batch_size': [47.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [973.0],\n",
    "  'dropout': [0.19862535182199834],\n",
    "  'gru_bias_reg': [1.011936859273273e-08],\n",
    "  'gru_dim': [258.0],\n",
    "  'gru_kernel_reg': [2.0678669679829352e-10],\n",
    "  'gru_recc_reg': [8.946942716621634e-07],\n",
    "  'lr1': [0.0015982451490776767],\n",
    "  'lr2': [0.0002459290205687559],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [3],\n",
    "  'spatial_drop': [0.3696100622336198]},\n",
    " {'batch_size': [83.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [351.0],\n",
    "  'dropout': [0.07833431778315075],\n",
    "  'gru_bias_reg': [1.989216237371643e-09],\n",
    "  'gru_dim': [278.0],\n",
    "  'gru_kernel_reg': [2.1606860352426398e-10],\n",
    "  'gru_recc_reg': [1.6736919208281796e-07],\n",
    "  'lr1': [0.00263784102869703],\n",
    "  'lr2': [0.0005711207564167526],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [3],\n",
    "  'spatial_drop': [0.31401382410917008]},\n",
    " {'batch_size': [49.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [997.0],\n",
    "  'dropout': [0.19115533803668047],\n",
    "  'gru_bias_reg': [5.222640591389245e-10],\n",
    "  'gru_dim': [299.0],\n",
    "  'gru_kernel_reg': [8.078459790975857e-10],\n",
    "  'gru_recc_reg': [6.100081276448957e-08],\n",
    "  'lr1': [0.0019427338445684181],\n",
    "  'lr2': [0.00010186610979091696],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [3],\n",
    "  'spatial_drop': [0.32614208466560007]},\n",
    " {'batch_size': [41.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [973.0],\n",
    "  'dropout': [0.20050865242539928],\n",
    "  'gru_bias_reg': [1.1451922219328368e-08],\n",
    "  'gru_dim': [292.0],\n",
    "  'gru_kernel_reg': [1.0516629869555607e-09],\n",
    "  'gru_recc_reg': [1.2593577396164419e-06],\n",
    "  'lr1': [0.0016205788115723873],\n",
    "  'lr2': [0.00011538601448660545],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [3],\n",
    "  'spatial_drop': [0.3803897135211322]},\n",
    " {'batch_size': [37.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [237.0],\n",
    "  'dropout': [0.12273937792021693],\n",
    "  'gru_bias_reg': [2.7055793227129377e-09],\n",
    "  'gru_dim': [307.0],\n",
    "  'gru_kernel_reg': [1.9122269544090935e-09],\n",
    "  'gru_recc_reg': [1.5269966614646778e-06],\n",
    "  'lr1': [0.0019545667587842147],\n",
    "  'lr2': [0.00034205962093229346],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [3],\n",
    "  'spatial_drop': [0.339366738134983]},\n",
    " {'batch_size': [16.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [783.0],\n",
    "  'dropout': [0.19251258375962352],\n",
    "  'gru_bias_reg': [1.6889374260262626e-08],\n",
    "  'gru_dim': [401.0],\n",
    "  'gru_kernel_reg': [2.1685591958268602e-10],\n",
    "  'gru_recc_reg': [8.179324804312695e-07],\n",
    "  'lr1': [0.0016526011543532724],\n",
    "  'lr2': [0.00020256532886638333],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [3],\n",
    "  'spatial_drop': [0.3994866402536531]},\n",
    " {'batch_size': [90.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [995.0],\n",
    "  'dropout': [0.22199634608987717],\n",
    "  'gru_bias_reg': [1.162462425352503e-10],\n",
    "  'gru_dim': [234.0],\n",
    "  'gru_kernel_reg': [2.6194833614316782e-09],\n",
    "  'gru_recc_reg': [3.719163247084088e-10],\n",
    "  'lr1': [0.002927034550684743],\n",
    "  'lr2': [0.0003517541152030026],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [3],\n",
    "  'spatial_drop': [0.3670368024684063]},\n",
    " {'batch_size': [71.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [999.0],\n",
    "  'dropout': [0.31884760544934115],\n",
    "  'gru_bias_reg': [3.489577142351314e-10],\n",
    "  'gru_dim': [310.0],\n",
    "  'gru_kernel_reg': [3.464891994757166e-10],\n",
    "  'gru_recc_reg': [1.5572206853920122e-09],\n",
    "  'lr1': [0.0019593704753874588],\n",
    "  'lr2': [0.000472320376364918],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [3],\n",
    "  'spatial_drop': [0.3905500602659617]},\n",
    " {'batch_size': [21.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [971.0],\n",
    "  'dropout': [0.07825915818812121],\n",
    "  'gru_bias_reg': [9.370609019263161e-08],\n",
    "  'gru_dim': [452.0],\n",
    "  'gru_kernel_reg': [1.1699932655152522e-09],\n",
    "  'gru_recc_reg': [1.118300667865804e-06],\n",
    "  'lr1': [0.0009194436554174992],\n",
    "  'lr2': [0.00011484392164348851],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [4],\n",
    "  'spatial_drop': [0.5127687159502792]},\n",
    " {'batch_size': [41.0],\n",
    "  'bidirectional': [1],\n",
    "  'dense_dim': [888.0],\n",
    "  'dropout': [0.18388910768345024],\n",
    "  'gru_bias_reg': [1.0198714543301587e-08],\n",
    "  'gru_dim': [322.0],\n",
    "  'gru_kernel_reg': [4.3029409259060245e-08],\n",
    "  'gru_recc_reg': [3.1764731171976037e-06],\n",
    "  'lr1': [0.0017624941021206772],\n",
    "  'lr2': [0.0006933312845721429],\n",
    "  'mask_zero': [1],\n",
    "  'optimizer': [0],\n",
    "  'pool_type': [2],\n",
    "  'spatial_drop': [0.4120345974541232]}\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_dic = {0:0.001, 1:0.001, 2:0.0009, 3:0.0008, 4:0.0007, 5:0.0006, 6:0.0005, 7:0.0004, 8:0.0003, 9:0.0002, 10:0.0001,\n",
    "            11:0.00009, 12:0.00008, 13:0.00007, 14:0.00006, 15:0.00005}\n",
    "def lr_decay(epoch):\n",
    "    return decay_dic[epoch]\n",
    "def shuffle_train_predict(model, cvlist, X, y, X_test, lr_decay):\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    y_test_preds = []\n",
    "    scores = []\n",
    "    now_nfold=0\n",
    "    LRDecay = LearningRateScheduler(lr_decay)\n",
    "    shuffle_word_pad = tr_word_pad[original_train.shape[0]:]\n",
    "    shuffle_word_pad_y = one_y_train[original_train.shape[0]:]\n",
    "    split_y = Y[original_train.shape[0]:]\n",
    "#     kf = KFold(n_splits=5, shuffle=False, random_state=233)\n",
    "    cvs = list(StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=786).split(shuffle_data, split_y))\n",
    "    for tr_index, val_index in cvlist:\n",
    "        now_nfold+=1\n",
    "        print (\"now is {} fold\".format(now_nfold))\n",
    "        X_tr, y_tr = X[tr_index, :], y[tr_index, :]\n",
    "        X_val, y_val = X[val_index, :], y[val_index, :]\n",
    "        shuffle_number = 1\n",
    "        for shuffle_train_index, shuffle_test_index in cvs:\n",
    "            shuffle_train = shuffle_word_pad[shuffle_train_index]\n",
    "            shuffle_train_y = shuffle_word_pad_y[shuffle_train_index]\n",
    "            if shuffle_number == now_nfold:\n",
    "                break\n",
    "            shuffle_number+=1\n",
    "        X_tr = np.vstack((X_tr, shuffle_train))\n",
    "        y_tr = np.vstack((y_tr, shuffle_train_y))\n",
    "        checkpoint = ModelCheckpoint(file_path, save_best_only=True,verbose=1, monitor='val_f1',  mode='max')\n",
    "        early = EarlyStopping( monitor='val_f1',  mode='max', patience=2,)\n",
    "\n",
    "        model.set_params(**{'callbacks':[checkpoint,early,LRDecay]})\n",
    "        model.fit(X_tr, y_tr, X_val, y_val)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "#         score = f1_score(y_val, y_pred,  average='macro')\n",
    "#         scores.append(score)\n",
    "#         print(\"f1 for this fold is \", score)\n",
    "        y_trues.append(y_val)\n",
    "        y_preds.append(y_pred)\n",
    "        y_test_preds.append(y_test_pred)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        #break\n",
    "    y_trues = np.concatenate(y_trues)\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "    y_test_preds = np.mean(y_test_preds, axis=0)\n",
    "    print(\"Shape of test _preds is \", y_test_preds.shape)\n",
    "    print(\"Means of val and test preds are {} and {}\".format(np.mean(y_preds, axis=1), np.mean(y_test_preds, axis=1)))\n",
    "    score = f1_score(y_trues, y_preds,  average='macro')\n",
    "    print(\"Overall score on 10 fold CV is {}\".format(score))\n",
    "    \n",
    "    return y_preds, y_trues, y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(parameter_space):\n",
    "    def lr_decay(epoch):\n",
    "        if epoch == 0:\n",
    "            return parameter_space['lr1'][0]\n",
    "        else:\n",
    "            return parameter_space['lr2'][0]\n",
    "    model = GRUClassifier(initial_weights=embedding_matrix_desc, bidirectional=[False, True][parameter_space['bidirectional'][0]],\n",
    "                          gru_dim = int(parameter_space['gru_dim'][0]),\n",
    "                          dense_dim = int(parameter_space['dense_dim'][0]),\n",
    "                          mask_zero = [True, False][parameter_space['mask_zero'][0]],\n",
    "                          pool_type = ['avg', 'max', 'attn',  'all', 'attn_avg'][parameter_space['pool_type'][0]],\n",
    "                          batch_size= int(parameter_space['batch_size'][0]), \n",
    "                          epochs=15,\n",
    "                          optimizer=[\"adam\", \"rmsprop\"][parameter_space['optimizer'][0]],\n",
    "                          dropout=parameter_space['dropout'][0],\n",
    "                          spatial_drop=parameter_space['spatial_drop'][0],\n",
    "                          gru_kernel_regularization = parameter_space[\"gru_kernel_reg\"][0],\n",
    "                          gru_recurrent_regularization = parameter_space[\"gru_recc_reg\"][0],\n",
    "                          gru_bias_regularization = parameter_space[\"gru_bias_reg\"][0],\n",
    "                          #embeddings_regularization = parameter_space[\"embeddings_reg\"],\n",
    "                          )\n",
    "\n",
    "    y_preds, y_trues, y_test_preds = shuffle_train_predict(model, cvlist2, tr_word_pad, one_y_train, te_word_pad, lr_decay) \n",
    "    return y_preds, y_trues, y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now is 1 fold\n",
      "Train on 111667 samples, validate on 11318 samples\n",
      "Epoch 1/15\n",
      "  3431/111667 [..............................] - ETA: 52:33 - loss: 2.0284 - f1: 0.2879"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-42353f7b72bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my_preds_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_trues_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_trues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-18ba69888ade>\u001b[0m in \u001b[0;36mtrain_predict\u001b[0;34m(parameter_space)\u001b[0m\n\u001b[1;32m     21\u001b[0m                           )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_train_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvlist2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_word_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_word_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-edba29cf10b6>\u001b[0m in \u001b[0;36mshuffle_train_predict\u001b[0;34m(model, cvlist, X, y, X_test, lr_decay)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLRDecay\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-afdf838c87f1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, hold_out_x1, hold_out_y)\u001b[0m\n\u001b[1;32m    115\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhold_out_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold_out_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                        shuffle=True)\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "y_preds_all = []\n",
    "y_trues_all = []\n",
    "y_test_preds_all = []\n",
    "rounds = 1\n",
    "for params in parameter_list:\n",
    "    y_preds, y_trues, y_test_preds = train_predict(params)\n",
    "    y_preds_all.append(y_preds)\n",
    "    y_trues_all.append(y_trues)\n",
    "    y_test_preds_all.append(y_test_preds)\n",
    "    with open('../feature/rnn_model_bagging{}'.format(rounds),'wb') as fout:\n",
    "        pickle.dump([y_preds,y_trues,y_test_preds],fout)\n",
    "    rounds+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../feature/rnn_model_bagging_final','wb') as fout:\n",
    "    pickle.dump([y_preds_all,y_trues_all,y_test_preds_all],fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
