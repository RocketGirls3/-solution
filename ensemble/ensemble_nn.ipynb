{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../feature/stacking_pkl_file/Capsule.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/capsule_all_shuffle.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/capsule_shuffle.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/cnn.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/mlp.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/mlp_shuffle.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/pseudo_rnn2.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/rnn7879.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/rnn_all_shuffle_all.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/rnn_cnn.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/rnn_pseudo1.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/rnn_shuffle1.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/stacking_pkl_file/rnn_shuffle2.pkl\n",
      "(102277, 19) (102277, 19)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "# import time\n",
    "# print('sleeping')\n",
    "# time.sleep(7200)\n",
    "# print('sleep done =======================')\n",
    "# load feats\n",
    "all = True\n",
    "if all:\n",
    "    train_x,test_x = [],[]\n",
    "    for feat in sorted(glob.glob('../feature/stacking_pkl_file/*')):\n",
    "        if 'fm2.pkl' in feat or 'ligbm' in  feat :\n",
    "            continue\n",
    "        print('file path',feat)\n",
    "        a,b = pickle.load(open(feat,'rb'))\n",
    "        print(a.shape,b.shape)\n",
    "        train_x.append(a)\n",
    "        test_x.append(b)\n",
    "else:\n",
    "    train_x,test_x = [],[]\n",
    "    for feat in sorted(glob.glob('../feature/staclomg_pkl_file_nn/*')):\n",
    "        if 'fm2.pkl' in feat or 'ligbm' in feat :\n",
    "            continue\n",
    "        print('file path',feat)\n",
    "        a,b = pickle.load(open(feat,'rb'))\n",
    "        print(a.shape,b.shape)\n",
    "        train_x.append(a)\n",
    "        test_x.append(b)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../feature/other_features/features-vinson/bnb_prob_10w.csv\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/other_features/features-vinson/lr_prob.csv\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/other_features/features-vinson/lr_prob_0.778620.csv\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/other_features/features-vinson/mnb_prob_10w.csv\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/other_features/features-vinson/nn_ensemble_0.775597.csv\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/other_features/features-vinson/svc_prob.csv\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/other_features/features-vinson/svc_prob_0.778881.csv\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/other_features/features-vinson/word_fasttext_0.760225.csv\n",
      "(102277, 19) (102277, 19)\n",
      "(102277, 399)\n"
     ]
    }
   ],
   "source": [
    "for feat in sorted(glob.glob('../feature/other_features/features-vinson/*')):\n",
    "    if 'fm2.pkl' in feat or 'w2v' in feat or 'lda' in feat or 'lsi' in feat:\n",
    "        continue\n",
    "    print('file path',feat)\n",
    "    file=pd.read_csv(feat)\n",
    "    a = file.iloc[:102277,:]\n",
    "    b = file.iloc[102277:,:]\n",
    "    a = a.values\n",
    "    b = b.values\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/new_data/train_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "y=(train[\"class\"]-1).astype(int)\n",
    "from keras.utils import np_utils\n",
    "train_y=np_utils.to_categorical(y,num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_f1_score(data, y_hat):\n",
    "    y_true = data\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return f1_score(y_true, y_hat, average='macro')\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Model\n",
    "def get_nn():\n",
    "    cv1_input = Input(shape=(399,), dtype='float32')\n",
    "#     merged = concatenate([cv1_input])\n",
    "    merged = Dense(512,activation='relu')(cv1_input)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    x = Dense(20, activation=\"softmax\")(merged)\n",
    "    model = Model(inputs=cv1_input, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    now_nfold=0\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    test_pred, train_pred = np.zeros((102277,20)),np.zeros((102277,20))\n",
    "#     LRDecay = LearningRateScheduler(lr_decay)\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        \n",
    "        now_nfold+=1\n",
    "        print (\"now is {} fold\".format(now_nfold))\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        d_test = test_x\n",
    "        batch_size = 64\n",
    "        epochs = 15\n",
    "\n",
    "        model = get_nn()\n",
    "\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, save_best_only=True,verbose=1, monitor='val_f1',  mode='max')\n",
    "        early = EarlyStopping( monitor='val_f1',  mode='max', patience=2,)\n",
    "        callbacks_list = [checkpoint,early]\n",
    "\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                    validation_data=(hold_out_x, hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "       \n",
    "        train_pred[test_index] = model.predict(hold_out_x)\n",
    "        curr_test_pred = model.predict(d_test)\n",
    "        test_pred += curr_test_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now is 1 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "92049/92049 [==============================] - 14s 153us/step - loss: 0.7196 - f1: 0.8062 - val_loss: 0.6805 - val_f1: 0.8154\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.81541, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.6593 - f1: 0.8157 - val_loss: 0.6578 - val_f1: 0.8161\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.81541 to 0.81610, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.6410 - f1: 0.8169 - val_loss: 0.6474 - val_f1: 0.8166\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.81610 to 0.81664, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.6323 - f1: 0.8187 - val_loss: 0.6487 - val_f1: 0.8138\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.81664\n",
      "Epoch 5/15\n",
      "92049/92049 [==============================] - 7s 75us/step - loss: 0.6239 - f1: 0.8189 - val_loss: 0.6440 - val_f1: 0.8157\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.81664\n",
      "now is 2 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.7213 - f1: 0.8055 - val_loss: 0.6507 - val_f1: 0.8205\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.82048, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92049/92049 [==============================] - 7s 76us/step - loss: 0.6618 - f1: 0.8158 - val_loss: 0.6316 - val_f1: 0.8151\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.82048\n",
      "Epoch 3/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.6444 - f1: 0.8161 - val_loss: 0.6303 - val_f1: 0.8157\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.82048\n",
      "now is 3 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "92049/92049 [==============================] - 7s 77us/step - loss: 0.7224 - f1: 0.8053 - val_loss: 0.6546 - val_f1: 0.8201\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.82014, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.6619 - f1: 0.8146 - val_loss: 0.6509 - val_f1: 0.8157\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.82014\n",
      "Epoch 3/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.6426 - f1: 0.8164 - val_loss: 0.6343 - val_f1: 0.8160\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.82014\n",
      "now is 4 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "92049/92049 [==============================] - 7s 76us/step - loss: 0.7211 - f1: 0.8055 - val_loss: 0.6563 - val_f1: 0.8159\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.81589, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92049/92049 [==============================] - 6s 70us/step - loss: 0.6619 - f1: 0.8152 - val_loss: 0.6361 - val_f1: 0.8198\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.81589 to 0.81979, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "92049/92049 [==============================] - 7s 71us/step - loss: 0.6447 - f1: 0.8162 - val_loss: 0.6332 - val_f1: 0.8211\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.81979 to 0.82109, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "92049/92049 [==============================] - 6s 70us/step - loss: 0.6350 - f1: 0.8177 - val_loss: 0.6239 - val_f1: 0.8198\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.82109\n",
      "Epoch 5/15\n",
      "92049/92049 [==============================] - 7s 71us/step - loss: 0.6265 - f1: 0.8191 - val_loss: 0.6220 - val_f1: 0.8234\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.82109 to 0.82336, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "92049/92049 [==============================] - 8s 83us/step - loss: 0.6222 - f1: 0.8197 - val_loss: 0.6236 - val_f1: 0.8204\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.82336\n",
      "Epoch 7/15\n",
      "92049/92049 [==============================] - 7s 79us/step - loss: 0.6168 - f1: 0.8204 - val_loss: 0.6190 - val_f1: 0.8192\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.82336\n",
      "now is 5 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "92049/92049 [==============================] - 7s 81us/step - loss: 0.7216 - f1: 0.8062 - val_loss: 0.6516 - val_f1: 0.8205\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.82051, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92049/92049 [==============================] - 7s 81us/step - loss: 0.6601 - f1: 0.8154 - val_loss: 0.6375 - val_f1: 0.8153\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.82051\n",
      "Epoch 3/15\n",
      "92049/92049 [==============================] - 6s 69us/step - loss: 0.6434 - f1: 0.8169 - val_loss: 0.6493 - val_f1: 0.8111\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.82051\n",
      "now is 6 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "92049/92049 [==============================] - 7s 73us/step - loss: 0.7206 - f1: 0.8062 - val_loss: 0.6815 - val_f1: 0.8117\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.81171, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92049/92049 [==============================] - 7s 72us/step - loss: 0.6597 - f1: 0.8160 - val_loss: 0.6580 - val_f1: 0.8207\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.81171 to 0.82072, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "92049/92049 [==============================] - 7s 72us/step - loss: 0.6432 - f1: 0.8163 - val_loss: 0.6498 - val_f1: 0.8153\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.82072\n",
      "Epoch 4/15\n",
      "92049/92049 [==============================] - 7s 72us/step - loss: 0.6325 - f1: 0.8178 - val_loss: 0.6367 - val_f1: 0.8164\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.82072\n",
      "now is 7 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.7182 - f1: 0.8064 - val_loss: 0.6740 - val_f1: 0.8135\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.81352, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92049/92049 [==============================] - 6s 70us/step - loss: 0.6578 - f1: 0.8155 - val_loss: 0.6625 - val_f1: 0.8094\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.81352\n",
      "Epoch 3/15\n",
      "92049/92049 [==============================] - 6s 70us/step - loss: 0.6410 - f1: 0.8169 - val_loss: 0.6576 - val_f1: 0.8146\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.81352 to 0.81464, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "92049/92049 [==============================] - 7s 78us/step - loss: 0.6316 - f1: 0.8180 - val_loss: 0.6523 - val_f1: 0.8164\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.81464 to 0.81644, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "92049/92049 [==============================] - 8s 83us/step - loss: 0.6252 - f1: 0.8190 - val_loss: 0.6473 - val_f1: 0.8166\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.81644 to 0.81662, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "92049/92049 [==============================] - 8s 82us/step - loss: 0.6201 - f1: 0.8193 - val_loss: 0.6400 - val_f1: 0.8160\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.81662\n",
      "Epoch 7/15\n",
      "92049/92049 [==============================] - 7s 71us/step - loss: 0.6148 - f1: 0.8201 - val_loss: 0.6471 - val_f1: 0.8184\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.81662 to 0.81840, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "92049/92049 [==============================] - 7s 73us/step - loss: 0.6094 - f1: 0.8205 - val_loss: 0.6425 - val_f1: 0.8170\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.81840\n",
      "Epoch 9/15\n",
      "92049/92049 [==============================] - 7s 72us/step - loss: 0.6055 - f1: 0.8216 - val_loss: 0.6434 - val_f1: 0.8180\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.81840\n",
      "now is 8 fold\n",
      "Train on 92050 samples, validate on 10227 samples\n",
      "Epoch 1/15\n",
      "92050/92050 [==============================] - 7s 73us/step - loss: 0.7190 - f1: 0.8055 - val_loss: 0.6737 - val_f1: 0.8167\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.81674, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92050/92050 [==============================] - 7s 72us/step - loss: 0.6583 - f1: 0.8151 - val_loss: 0.6543 - val_f1: 0.8163\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.81674\n",
      "Epoch 3/15\n",
      "92050/92050 [==============================] - 7s 74us/step - loss: 0.6435 - f1: 0.8160 - val_loss: 0.6438 - val_f1: 0.8185\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.81674 to 0.81847, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92050/92050 [==============================] - 7s 77us/step - loss: 0.6330 - f1: 0.8181 - val_loss: 0.6366 - val_f1: 0.8191\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.81847 to 0.81907, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "92050/92050 [==============================] - 7s 76us/step - loss: 0.6264 - f1: 0.8182 - val_loss: 0.6366 - val_f1: 0.8215\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.81907 to 0.82148, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "92050/92050 [==============================] - 7s 73us/step - loss: 0.6200 - f1: 0.8195 - val_loss: 0.6268 - val_f1: 0.8209\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.82148\n",
      "Epoch 7/15\n",
      "92050/92050 [==============================] - 7s 71us/step - loss: 0.6155 - f1: 0.8194 - val_loss: 0.6306 - val_f1: 0.8213\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.82148\n",
      "now is 9 fold\n",
      "Train on 92050 samples, validate on 10227 samples\n",
      "Epoch 1/15\n",
      "92050/92050 [==============================] - 7s 73us/step - loss: 0.7134 - f1: 0.8080 - val_loss: 0.7135 - val_f1: 0.8063\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.80627, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92050/92050 [==============================] - 6s 69us/step - loss: 0.6543 - f1: 0.8185 - val_loss: 0.6937 - val_f1: 0.8029\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.80627\n",
      "Epoch 3/15\n",
      "92050/92050 [==============================] - 6s 70us/step - loss: 0.6383 - f1: 0.8186 - val_loss: 0.6786 - val_f1: 0.8049\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.80627\n",
      "now is 10 fold\n",
      "Train on 92050 samples, validate on 10227 samples\n",
      "Epoch 1/15\n",
      "92050/92050 [==============================] - 7s 73us/step - loss: 0.7191 - f1: 0.8060 - val_loss: 0.6749 - val_f1: 0.8163\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.81628, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "92050/92050 [==============================] - 6s 70us/step - loss: 0.6597 - f1: 0.8154 - val_loss: 0.6490 - val_f1: 0.8177\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.81628 to 0.81774, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "92050/92050 [==============================] - 7s 71us/step - loss: 0.6425 - f1: 0.8169 - val_loss: 0.6429 - val_f1: 0.8175\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.81774\n",
      "Epoch 4/15\n",
      "92050/92050 [==============================] - 7s 76us/step - loss: 0.6320 - f1: 0.8186 - val_loss: 0.6385 - val_f1: 0.8181\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.81774 to 0.81812, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "92050/92050 [==============================] - 7s 80us/step - loss: 0.6262 - f1: 0.8191 - val_loss: 0.6372 - val_f1: 0.8191\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.81812 to 0.81910, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "92050/92050 [==============================] - 7s 80us/step - loss: 0.6204 - f1: 0.8193 - val_loss: 0.6350 - val_f1: 0.8132\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.81910\n",
      "Epoch 7/15\n",
      "92050/92050 [==============================] - 7s 77us/step - loss: 0.6135 - f1: 0.8212 - val_loss: 0.6375 - val_f1: 0.8133\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.81910\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_pred, nn_res = kf_train(fold_cnt=10,rnd=4)\n",
    "#### first fold #######8141 0.796<lb<0.797 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../feature/2leve_nn_stacking2.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,nn_res],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../input/new_data/test_set.csv'\n",
    "test = pd.read_csv(test_dir)\n",
    "test_id = test[[\"id\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102277, 1)\n",
      "(102277, 1)\n"
     ]
    }
   ],
   "source": [
    "preds=np.argmax(nn_res,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"class\"]\n",
    "test_pred[\"class\"]=(test_pred[\"class\"]+1).astype(int)\n",
    "print(test_pred.shape)\n",
    "print(test_id.shape)\n",
    "test_pred[\"id\"]=list(test_id[\"id\"])\n",
    "test_pred[[\"id\",\"class\"]].to_csv('../output/nnstack5.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = [\"class_prob_%s\"%i for i in range(1,21)]\n",
    "df_nn = pd.DataFrame(nn_res, columns=name)\n",
    "df_nn = df_nn.drop('class_prob_20', axis=1)\n",
    "df_nn.to_csv('../pro/stacking_nn5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102277, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_prob_1</th>\n",
       "      <th>class_prob_2</th>\n",
       "      <th>class_prob_3</th>\n",
       "      <th>class_prob_4</th>\n",
       "      <th>class_prob_5</th>\n",
       "      <th>class_prob_6</th>\n",
       "      <th>class_prob_7</th>\n",
       "      <th>class_prob_8</th>\n",
       "      <th>class_prob_9</th>\n",
       "      <th>class_prob_10</th>\n",
       "      <th>class_prob_11</th>\n",
       "      <th>class_prob_12</th>\n",
       "      <th>class_prob_13</th>\n",
       "      <th>class_prob_14</th>\n",
       "      <th>class_prob_15</th>\n",
       "      <th>class_prob_16</th>\n",
       "      <th>class_prob_17</th>\n",
       "      <th>class_prob_18</th>\n",
       "      <th>class_prob_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.985771</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.984887</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020216</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.020515</td>\n",
       "      <td>0.003525</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>0.004055</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.066945</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.032441</td>\n",
       "      <td>0.780038</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.017775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.996653</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.963366</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.002322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022627</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.919910</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.007751</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.002178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.967127</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.005398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.035851</td>\n",
       "      <td>0.157296</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.008245</td>\n",
       "      <td>0.063860</td>\n",
       "      <td>0.019170</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.017275</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>0.008736</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.066895</td>\n",
       "      <td>0.025969</td>\n",
       "      <td>0.508186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.994240</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.087528</td>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.020393</td>\n",
       "      <td>0.177747</td>\n",
       "      <td>0.018016</td>\n",
       "      <td>0.073854</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>0.473774</td>\n",
       "      <td>0.020344</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.055936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.064365</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.302933</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.239870</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.341484</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.006266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.293046</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.246669</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.058060</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.021792</td>\n",
       "      <td>0.027306</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.024547</td>\n",
       "      <td>0.020159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.988428</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.015354</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.065278</td>\n",
       "      <td>0.850891</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>0.020962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.997962</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.990857</td>\n",
       "      <td>0.004139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.998090</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.001072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.707426</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.258823</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.002510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.691492</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.280892</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.003650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.993454</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.001641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.053909</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.026511</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>0.016823</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.442478</td>\n",
       "      <td>0.251804</td>\n",
       "      <td>0.056311</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.028431</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.007278</td>\n",
       "      <td>0.007544</td>\n",
       "      <td>0.039950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.999216</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.056648</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.091933</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.727203</td>\n",
       "      <td>0.005806</td>\n",
       "      <td>0.022029</td>\n",
       "      <td>0.010473</td>\n",
       "      <td>0.012999</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.042311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.926281</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.014365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.987349</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.751311</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.231721</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.001962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.013387</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.003620</td>\n",
       "      <td>0.015035</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.498019</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.377569</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.013123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.009263</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.958956</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.002336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.006260</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>0.957313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102247</th>\n",
       "      <td>0.016951</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.948355</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102248</th>\n",
       "      <td>0.012718</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.951175</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.001820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102249</th>\n",
       "      <td>0.018291</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.099664</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102250</th>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.988216</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.001221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102251</th>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.992540</td>\n",
       "      <td>0.004282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102252</th>\n",
       "      <td>0.037137</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.007520</td>\n",
       "      <td>0.034208</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.179223</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.025314</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.689031</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.004421</td>\n",
       "      <td>0.003806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102253</th>\n",
       "      <td>0.015474</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.953788</td>\n",
       "      <td>0.015412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102254</th>\n",
       "      <td>0.007940</td>\n",
       "      <td>0.952967</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.008512</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.005824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102255</th>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.887350</td>\n",
       "      <td>0.013845</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.002782</td>\n",
       "      <td>0.008004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102256</th>\n",
       "      <td>0.986727</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102257</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.996047</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102258</th>\n",
       "      <td>0.004780</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.013765</td>\n",
       "      <td>0.007609</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.912605</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.024512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102259</th>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.028141</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.947051</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.002838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102260</th>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.019483</td>\n",
       "      <td>0.939984</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.002191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102261</th>\n",
       "      <td>0.025996</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.044995</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>0.029107</td>\n",
       "      <td>0.015885</td>\n",
       "      <td>0.012781</td>\n",
       "      <td>0.013466</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.138334</td>\n",
       "      <td>0.135907</td>\n",
       "      <td>0.042590</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>0.051232</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.010423</td>\n",
       "      <td>0.075568</td>\n",
       "      <td>0.348201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102262</th>\n",
       "      <td>0.007198</td>\n",
       "      <td>0.935334</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.015011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102263</th>\n",
       "      <td>0.006403</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.022191</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.008949</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.896637</td>\n",
       "      <td>0.009241</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.013063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102264</th>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.985437</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102265</th>\n",
       "      <td>0.174008</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.020433</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.753781</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.005265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102266</th>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.019236</td>\n",
       "      <td>0.047767</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>0.017437</td>\n",
       "      <td>0.020560</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.163452</td>\n",
       "      <td>0.025323</td>\n",
       "      <td>0.028918</td>\n",
       "      <td>0.019349</td>\n",
       "      <td>0.023478</td>\n",
       "      <td>0.231075</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.013712</td>\n",
       "      <td>0.033957</td>\n",
       "      <td>0.195939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102267</th>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.992324</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102268</th>\n",
       "      <td>0.041001</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.927049</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.007444</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.002754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102269</th>\n",
       "      <td>0.072642</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.022310</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.845225</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.003644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102270</th>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.006649</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.986814</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102271</th>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.994374</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102272</th>\n",
       "      <td>0.018286</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.820742</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.032397</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.004504</td>\n",
       "      <td>0.058428</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.017115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102273</th>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.006609</td>\n",
       "      <td>0.006906</td>\n",
       "      <td>0.015392</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.534510</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.020079</td>\n",
       "      <td>0.278031</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.057056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102274</th>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.006110</td>\n",
       "      <td>0.079656</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.887829</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102275</th>\n",
       "      <td>0.012120</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.004557</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.948591</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.002215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102276</th>\n",
       "      <td>0.037960</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.011499</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0.011457</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.845611</td>\n",
       "      <td>0.041805</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.018055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102277 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_prob_1  class_prob_2  class_prob_3  class_prob_4  class_prob_5  \\\n",
       "0           0.002993      0.000245      0.001125      0.000995      0.985771   \n",
       "1           0.004403      0.000646      0.000334      0.984887      0.000505   \n",
       "2           0.020216      0.003476      0.020515      0.003525      0.006963   \n",
       "3           0.000698      0.000111      0.000133      0.996653      0.000171   \n",
       "4           0.012177      0.000401      0.001422      0.001217      0.963366   \n",
       "5           0.022627      0.001485      0.005433      0.002175      0.919910   \n",
       "6           0.005423      0.000255      0.003822      0.000366      0.000080   \n",
       "7           0.035851      0.157296      0.008682      0.005366      0.006395   \n",
       "8           0.000337      0.000018      0.994240      0.000084      0.000059   \n",
       "9           0.087528      0.004125      0.003935      0.005921      0.005538   \n",
       "10          0.064365      0.001599      0.001387      0.001863      0.005449   \n",
       "11          0.293046      0.211765      0.014287      0.004870      0.025306   \n",
       "12          0.005292      0.000214      0.000358      0.000369      0.988428   \n",
       "13          0.015354      0.002258      0.003658      0.002748      0.001720   \n",
       "14          0.000342      0.000012      0.000153      0.000020      0.000004   \n",
       "15          0.001510      0.000686      0.000204      0.000060      0.000020   \n",
       "16          0.000257      0.000006      0.000272      0.000010      0.000022   \n",
       "17          0.000766      0.000019      0.000461      0.000023      0.000046   \n",
       "18          0.001351      0.000278      0.707426      0.001349      0.000613   \n",
       "19          0.003352      0.000332      0.000651      0.002446      0.000270   \n",
       "20          0.001122      0.000027      0.000444      0.000030      0.000058   \n",
       "21          0.053909      0.002208      0.026511      0.005388      0.003906   \n",
       "22          0.000088      0.000002      0.000074      0.000003      0.000007   \n",
       "23          0.056648      0.000710      0.005473      0.000871      0.001216   \n",
       "24          0.007428      0.926281      0.004882      0.008110      0.004142   \n",
       "25          0.001662      0.000133      0.001337      0.000092      0.000024   \n",
       "26          0.001063      0.000086      0.751311      0.000581      0.000302   \n",
       "27          0.013387      0.003801      0.005226      0.005638      0.000868   \n",
       "28          0.009263      0.001170      0.000875      0.958956      0.001209   \n",
       "29          0.006260      0.001357      0.000505      0.000260      0.000526   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "102247      0.016951      0.000027      0.000049      0.000516      0.000080   \n",
       "102248      0.012718      0.000907      0.000876      0.000249      0.001441   \n",
       "102249      0.018291      0.000686      0.000444      0.000490      0.001764   \n",
       "102250      0.001714      0.988216      0.000474      0.001097      0.000706   \n",
       "102251      0.000981      0.000515      0.000109      0.000053      0.000013   \n",
       "102252      0.037137      0.001086      0.001395      0.007520      0.034208   \n",
       "102253      0.015474      0.002317      0.000586      0.000286      0.000138   \n",
       "102254      0.007940      0.952967      0.001943      0.003112      0.003429   \n",
       "102255      0.018115      0.000463      0.003550      0.004363      0.000907   \n",
       "102256      0.986727      0.000168      0.000428      0.000096      0.001729   \n",
       "102257      0.000108      0.000035      0.001413      0.000236      0.000071   \n",
       "102258      0.004780      0.000754      0.005785      0.002661      0.001404   \n",
       "102259      0.002134      0.000234      0.006475      0.000279      0.000101   \n",
       "102260      0.012513      0.000132      0.000547      0.003090      0.000401   \n",
       "102261      0.025996      0.021779      0.044995      0.008998      0.005930   \n",
       "102262      0.007198      0.935334      0.001726      0.006332      0.004878   \n",
       "102263      0.006403      0.001765      0.007721      0.002867      0.002240   \n",
       "102264      0.002229      0.000100      0.001927      0.000134      0.000035   \n",
       "102265      0.174008      0.000858      0.001220      0.002606      0.000992   \n",
       "102266      0.122011      0.019236      0.047767      0.004064      0.004889   \n",
       "102267      0.000483      0.000027      0.992324      0.000113      0.000080   \n",
       "102268      0.041001      0.001380      0.001673      0.000131      0.002434   \n",
       "102269      0.072642      0.000623      0.001382      0.004771      0.012553   \n",
       "102270      0.000206      0.000094      0.006649      0.000461      0.000175   \n",
       "102271      0.000788      0.000032      0.000726      0.000049      0.000010   \n",
       "102272      0.018286      0.009244      0.005489      0.001057      0.004474   \n",
       "102273      0.016052      0.006609      0.006906      0.015392      0.004734   \n",
       "102274      0.000922      0.000720      0.006110      0.079656      0.002540   \n",
       "102275      0.012120      0.000139      0.000533      0.002012      0.000406   \n",
       "102276      0.037960      0.003377      0.003097      0.001735      0.002802   \n",
       "\n",
       "        class_prob_6  class_prob_7  class_prob_8  class_prob_9  class_prob_10  \\\n",
       "0           0.000752      0.000198      0.000876      0.000093       0.001269   \n",
       "1           0.000564      0.001873      0.000194      0.000068       0.000761   \n",
       "2           0.004055      0.005920      0.066945      0.004388       0.005421   \n",
       "3           0.000301      0.000465      0.000091      0.000012       0.000174   \n",
       "4           0.001113      0.001120      0.001980      0.000238       0.005025   \n",
       "5           0.003880      0.000598      0.007751      0.000745       0.013252   \n",
       "6           0.001282      0.000477      0.001258      0.000284       0.005662   \n",
       "7           0.008245      0.063860      0.019170      0.008383       0.017275   \n",
       "8           0.001662      0.000078      0.000039      0.000065       0.000176   \n",
       "9           0.005220      0.020393      0.177747      0.018016       0.073854   \n",
       "10          0.001350      0.004968      0.302933      0.001153       0.239870   \n",
       "11          0.012292      0.004635      0.246669      0.006338       0.058060   \n",
       "12          0.000299      0.000064      0.000667      0.000048       0.001141   \n",
       "13          0.003084      0.002196      0.007515      0.001506       0.004160   \n",
       "14          0.000062      0.000023      0.000095      0.000017       0.000213   \n",
       "15          0.000101      0.000020      0.000154      0.000419       0.000353   \n",
       "16          0.000101      0.000117      0.000039      0.998090       0.000047   \n",
       "17          0.000175      0.000299      0.000082      0.994964       0.000112   \n",
       "18          0.011059      0.001801      0.000444      0.000922       0.001982   \n",
       "19          0.000288      0.004892      0.002224      0.000357       0.003701   \n",
       "20          0.000210      0.000555      0.000070      0.993454       0.000138   \n",
       "21          0.008952      0.016823      0.009362      0.027273       0.004469   \n",
       "22          0.000043      0.000055      0.000016      0.999216       0.000018   \n",
       "23          0.002023      0.091933      0.002216      0.727203       0.005806   \n",
       "24          0.001375      0.008217      0.002719      0.000302       0.001296   \n",
       "25          0.000600      0.000187      0.000350      0.000117       0.000663   \n",
       "26          0.006234      0.000605      0.000157      0.000209       0.001092   \n",
       "27          0.003620      0.015035      0.005419      0.002600       0.007935   \n",
       "28          0.001370      0.003527      0.000930      0.000189       0.002774   \n",
       "29          0.000476      0.002943      0.005397      0.001050       0.001589   \n",
       "...              ...           ...           ...           ...            ...   \n",
       "102247      0.000030      0.000013      0.031885      0.000014       0.000474   \n",
       "102248      0.000558      0.000349      0.951175      0.000557       0.016175   \n",
       "102249      0.001551      0.001359      0.099664      0.000228       0.853553   \n",
       "102250      0.000166      0.001142      0.000649      0.000027       0.000194   \n",
       "102251      0.000038      0.000015      0.000102      0.000256       0.000157   \n",
       "102252      0.001761      0.000522      0.179223      0.000529       0.025314   \n",
       "102253      0.000315      0.000132      0.003378      0.001058       0.001528   \n",
       "102254      0.000730      0.002525      0.008512      0.000130       0.001172   \n",
       "102255      0.001216      0.005188      0.007750      0.001476       0.013782   \n",
       "102256      0.000299      0.000075      0.001120      0.000102       0.000817   \n",
       "102257      0.996047      0.000025      0.000196      0.000009       0.000672   \n",
       "102258      0.001483      0.013765      0.007609      0.003847       0.003415   \n",
       "102259      0.001658      0.000442      0.000781      0.000730       0.000902   \n",
       "102260      0.000272      0.001553      0.005381      0.000207       0.006183   \n",
       "102261      0.029107      0.015885      0.012781      0.013466       0.007837   \n",
       "102262      0.000586      0.006162      0.004033      0.000159       0.000784   \n",
       "102263      0.002915      0.022191      0.005902      0.005741       0.008949   \n",
       "102264      0.000680      0.000197      0.000630      0.000211       0.001272   \n",
       "102265      0.000824      0.001971      0.009392      0.000550       0.020433   \n",
       "102266      0.016695      0.017437      0.020560      0.009204       0.163452   \n",
       "102267      0.001777      0.000125      0.000051      0.000063       0.000224   \n",
       "102268      0.000548      0.000349      0.927049      0.000821       0.007444   \n",
       "102269      0.000743      0.006504      0.004237      0.000660       0.022310   \n",
       "102270      0.986814      0.000076      0.000303      0.000048       0.001017   \n",
       "102271      0.000231      0.000091      0.000185      0.000089       0.000504   \n",
       "102272      0.003732      0.001811      0.820742      0.003539       0.032397   \n",
       "102273      0.008439      0.534510      0.005854      0.009488       0.007283   \n",
       "102274      0.887829      0.000689      0.003248      0.000168       0.011655   \n",
       "102275      0.000292      0.001736      0.004557      0.000301       0.014674   \n",
       "102276      0.002289      0.001601      0.011499      0.001915       0.011457   \n",
       "\n",
       "        class_prob_11  class_prob_12  class_prob_13  class_prob_14  \\\n",
       "0            0.000546       0.001497       0.000791       0.000891   \n",
       "1            0.000535       0.001523       0.000781       0.000965   \n",
       "2            0.012484       0.032441       0.780038       0.003759   \n",
       "3            0.000136       0.000225       0.000198       0.000262   \n",
       "4            0.001445       0.001602       0.002569       0.002457   \n",
       "5            0.004102       0.005181       0.003891       0.001761   \n",
       "6            0.001915       0.002070       0.000461       0.001747   \n",
       "7            0.009025       0.018115       0.020013       0.008736   \n",
       "8            0.000367       0.001420       0.000250       0.000192   \n",
       "9            0.013229       0.473774       0.020344       0.003572   \n",
       "10           0.000445       0.001434       0.009787       0.341484   \n",
       "11           0.008340       0.021792       0.027306       0.005210   \n",
       "12           0.000182       0.000503       0.000631       0.000364   \n",
       "13           0.004083       0.065278       0.850891       0.008264   \n",
       "14           0.000250       0.000067       0.000051       0.000353   \n",
       "15           0.000221       0.000450       0.000223       0.000388   \n",
       "16           0.000097       0.000130       0.000172       0.000105   \n",
       "17           0.000319       0.000352       0.000346       0.000302   \n",
       "18           0.004193       0.258823       0.001673       0.000663   \n",
       "19           0.000333       0.000854       0.001722       0.691492   \n",
       "20           0.000328       0.000433       0.000357       0.000430   \n",
       "21           0.442478       0.251804       0.056311       0.005033   \n",
       "22           0.000025       0.000048       0.000059       0.000036   \n",
       "23           0.022029       0.010473       0.012999       0.007497   \n",
       "24           0.000962       0.004546       0.002322       0.003030   \n",
       "25           0.003958       0.000581       0.000240       0.000770   \n",
       "26           0.001056       0.231721       0.001093       0.000347   \n",
       "27           0.010492       0.006632       0.005533       0.021164   \n",
       "28           0.002202       0.007553       0.002006       0.002611   \n",
       "29           0.000417       0.001375       0.001583       0.001049   \n",
       "...               ...            ...            ...            ...   \n",
       "102247       0.000010       0.000135       0.000837       0.000105   \n",
       "102248       0.000140       0.001137       0.007873       0.001069   \n",
       "102249       0.000177       0.001036       0.002751       0.001249   \n",
       "102250       0.000141       0.000897       0.000796       0.000851   \n",
       "102251       0.000107       0.000302       0.000164       0.000247   \n",
       "102252       0.000400       0.003191       0.006590       0.002472   \n",
       "102253       0.000553       0.001798       0.001389       0.000877   \n",
       "102254       0.000375       0.002622       0.002187       0.001622   \n",
       "102255       0.004724       0.008823       0.008353       0.887350   \n",
       "102256       0.000538       0.000450       0.002473       0.003048   \n",
       "102257       0.000035       0.000726       0.000237       0.000042   \n",
       "102258       0.003106       0.912605       0.008983       0.001332   \n",
       "102259       0.028141       0.005308       0.000746       0.000884   \n",
       "102260       0.000647       0.001509       0.019483       0.939984   \n",
       "102261       0.138334       0.135907       0.042590       0.008567   \n",
       "102262       0.000583       0.002737       0.002128       0.001908   \n",
       "102263       0.006831       0.896637       0.009241       0.001912   \n",
       "102264       0.002431       0.001226       0.000289       0.001074   \n",
       "102265       0.001662       0.010617       0.005502       0.753781   \n",
       "102266       0.025323       0.028918       0.019349       0.023478   \n",
       "102267       0.000289       0.002827       0.000164       0.000176   \n",
       "102268       0.000350       0.000729       0.010381       0.000540   \n",
       "102269       0.001296       0.001476       0.007979       0.845225   \n",
       "102270       0.000197       0.003043       0.000277       0.000135   \n",
       "102271       0.000800       0.000645       0.000147       0.000404   \n",
       "102272       0.001348       0.004504       0.058428       0.003187   \n",
       "102273       0.020079       0.278031       0.008217       0.006885   \n",
       "102274       0.000287       0.003234       0.000836       0.000479   \n",
       "102275       0.000473       0.000970       0.003240       0.948591   \n",
       "102276       0.002596       0.005952       0.845611       0.041805   \n",
       "\n",
       "        class_prob_15  class_prob_16  class_prob_17  class_prob_18  \\\n",
       "0            0.000256       0.000327       0.000184       0.000359   \n",
       "1            0.000206       0.000159       0.000131       0.000597   \n",
       "2            0.005350       0.000229       0.003238       0.003258   \n",
       "3            0.000052       0.000030       0.000051       0.000049   \n",
       "4            0.000358       0.000348       0.000299       0.000540   \n",
       "5            0.000876       0.001060       0.000546       0.002546   \n",
       "6            0.967127       0.000248       0.001203       0.000921   \n",
       "7            0.011118       0.001392       0.066895       0.025969   \n",
       "8            0.000417       0.000045       0.000024       0.000132   \n",
       "9            0.002183       0.005326       0.002417       0.020929   \n",
       "10           0.002141       0.008355       0.003844       0.001301   \n",
       "11           0.006871       0.003024       0.005456       0.024547   \n",
       "12           0.000122       0.000619       0.000079       0.000213   \n",
       "13           0.000928       0.000899       0.001215       0.003282   \n",
       "14           0.997962       0.000011       0.000089       0.000070   \n",
       "15           0.000092       0.000060       0.000042       0.990857   \n",
       "16           0.000163       0.000004       0.000016       0.000037   \n",
       "17           0.000412       0.000009       0.000042       0.000201   \n",
       "18           0.002917       0.000859       0.000399       0.000737   \n",
       "19           0.001017       0.001093       0.280892       0.000434   \n",
       "20           0.000351       0.000010       0.000051       0.000292   \n",
       "21           0.028431       0.002361       0.007278       0.007544   \n",
       "22           0.000048       0.000001       0.000004       0.000018   \n",
       "23           0.005136       0.000221       0.001711       0.003524   \n",
       "24           0.000782       0.000724       0.004101       0.004414   \n",
       "25           0.987349       0.000027       0.000332       0.000271   \n",
       "26           0.000665       0.000948       0.000104       0.000464   \n",
       "27           0.498019       0.000894       0.377569       0.003034   \n",
       "28           0.000751       0.000783       0.000343       0.001151   \n",
       "29           0.000424       0.000257       0.001537       0.015682   \n",
       "...               ...            ...            ...            ...   \n",
       "102247       0.000022       0.948355       0.000020       0.000205   \n",
       "102248       0.000477       0.001357       0.000343       0.000780   \n",
       "102249       0.000217       0.012355       0.000609       0.001022   \n",
       "102250       0.000145       0.000069       0.001187       0.000309   \n",
       "102251       0.000032       0.000050       0.000035       0.992540   \n",
       "102252       0.000665       0.689031       0.000724       0.004421   \n",
       "102253       0.000253       0.000443       0.000276       0.953788   \n",
       "102254       0.000411       0.000687       0.002687       0.001125   \n",
       "102255       0.013845       0.001099       0.008211       0.002782   \n",
       "102256       0.000184       0.000294       0.000131       0.000462   \n",
       "102257       0.000028       0.000019       0.000005       0.000021   \n",
       "102258       0.000509       0.001921       0.000825       0.000703   \n",
       "102259       0.947051       0.000042       0.000667       0.000587   \n",
       "102260       0.000991       0.001764       0.002690       0.000463   \n",
       "102261       0.051232       0.002370       0.010423       0.075568   \n",
       "102262       0.000413       0.000333       0.008273       0.001422   \n",
       "102263       0.000895       0.001930       0.001451       0.001344   \n",
       "102264       0.985437       0.000087       0.000389       0.000344   \n",
       "102265       0.002140       0.003309       0.002191       0.002676   \n",
       "102266       0.231075       0.002867       0.013712       0.033957   \n",
       "102267       0.000275       0.000146       0.000030       0.000223   \n",
       "102268       0.000459       0.000498       0.000300       0.001159   \n",
       "102269       0.002640       0.002797       0.007229       0.001286   \n",
       "102270       0.000067       0.000044       0.000020       0.000123   \n",
       "102271       0.994374       0.000023       0.000150       0.000112   \n",
       "102272       0.002629       0.001370       0.001468       0.009177   \n",
       "102273       0.004925       0.000914       0.003729       0.004885   \n",
       "102274       0.000239       0.000212       0.000154       0.000355   \n",
       "102275       0.001794       0.002212       0.003065       0.000669   \n",
       "102276       0.001830       0.001632       0.002122       0.002663   \n",
       "\n",
       "        class_prob_19  \n",
       "0            0.000833  \n",
       "1            0.000868  \n",
       "2            0.017775  \n",
       "3            0.000187  \n",
       "4            0.002322  \n",
       "5            0.002178  \n",
       "6            0.005398  \n",
       "7            0.508186  \n",
       "8            0.000395  \n",
       "9            0.055936  \n",
       "10           0.006266  \n",
       "11           0.020159  \n",
       "12           0.000405  \n",
       "13           0.020962  \n",
       "14           0.000207  \n",
       "15           0.004139  \n",
       "16           0.000315  \n",
       "17           0.001072  \n",
       "18           0.002510  \n",
       "19           0.003650  \n",
       "20           0.001641  \n",
       "21           0.039950  \n",
       "22           0.000239  \n",
       "23           0.042311  \n",
       "24           0.014365  \n",
       "25           0.001306  \n",
       "26           0.001962  \n",
       "27           0.013123  \n",
       "28           0.002336  \n",
       "29           0.957313  \n",
       "...               ...  \n",
       "102247       0.000272  \n",
       "102248       0.001820  \n",
       "102249       0.002551  \n",
       "102250       0.001221  \n",
       "102251       0.004282  \n",
       "102252       0.003806  \n",
       "102253       0.015412  \n",
       "102254       0.005824  \n",
       "102255       0.008004  \n",
       "102256       0.000859  \n",
       "102257       0.000075  \n",
       "102258       0.024512  \n",
       "102259       0.002838  \n",
       "102260       0.002191  \n",
       "102261       0.348201  \n",
       "102262       0.015011  \n",
       "102263       0.013063  \n",
       "102264       0.001306  \n",
       "102265       0.005265  \n",
       "102266       0.195939  \n",
       "102267       0.000602  \n",
       "102268       0.002754  \n",
       "102269       0.003644  \n",
       "102270       0.000250  \n",
       "102271       0.000642  \n",
       "102272       0.017115  \n",
       "102273       0.057056  \n",
       "102274       0.000668  \n",
       "102275       0.002215  \n",
       "102276       0.018055  \n",
       "\n",
       "[102277 rows x 19 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.70125763e-03, 9.41594957e-04, 4.31315157e-04, 1.03147708e-03,\n",
       "       9.80580300e-01, 6.71539639e-04, 4.68012059e-04, 2.37537872e-03,\n",
       "       1.03082371e-04, 1.38701548e-03, 9.77989039e-04, 2.48241170e-03,\n",
       "       4.65297431e-04, 5.17155064e-04, 3.12560847e-04, 1.24375290e-03,\n",
       "       1.98021449e-04, 3.04372875e-04, 8.06763067e-04, 7.34517270e-07])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
