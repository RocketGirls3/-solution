{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from collections import Counter\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Input, Embedding, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import optimizers, losses, activations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from models_def import Attention\n",
    "config = argparse.Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "der=True\n",
    "if der:\n",
    "    train_dir = '../input/new_data/train_set.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "else:\n",
    "    train_dir = '../input/train2.csv'\n",
    "    test_dir = '../input/test2.csv'\n",
    "train = pd.read_csv(train_dir)\n",
    "test = pd.read_csv(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clear_text(df, num, col):\n",
    "    word_seg = df[col]\n",
    "    word_seg_list = word_seg.apply(lambda x: x.split(\" \"))\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for line in word_seg_list:\n",
    "        word_counts.update(line)\n",
    "\n",
    "    counter_list = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    label = list(map(lambda x: x[0], counter_list[:num]))\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(label):\n",
    "        for j in word_seg_list:\n",
    "            if i in j:\n",
    "                j.remove(i)\n",
    "    df[col] = word_seg_list\n",
    "    df[col] = df[col].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "train_offset = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "all_data['word_len'] = all_data['word_seg'].apply(len)\n",
    "all_data['word_unique'] = all_data['word_seg'].apply(lambda comment: len(set( w for w in comment.split())))\n",
    "all_data['word_unique_vs_len'] = all_data['word_unique'] /  all_data['word_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data.iloc[:train_offset,:]\n",
    "test = all_data.iloc[train_offset:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "features = train['word_unique_vs_len'].fillna(0)\n",
    "test_features = test['word_unique_vs_len'].fillna(0)\n",
    "features = features.reshape(-1, 1)\n",
    "test_features = test_features.reshape(-1, 1)\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_train = (train[\"class\"]-1).astype(int)\n",
    "# column = \"word_seg\"\n",
    "test_id = test[[\"id\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['text'] = all_data['word_seg'].str.cat([all_data.article], sep=' ',na_rep='')\n",
    "all_data.drop(['word_seg', 'article'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data.iloc[:train_offset,:]\n",
    "test = all_data.iloc[train_offset:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"text\"\n",
    "config.len_desc = 500000\n",
    "tknzr_word = Tokenizer(num_words=config.len_desc)\n",
    "tknzr_word.fit_on_texts(all_data[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 849060/849060 [00:01<00:00, 747505.21it/s]\n"
     ]
    }
   ],
   "source": [
    "count_thres = 4\n",
    "low_count_words = [w for w, c in tknzr_word.word_counts.items() if c < count_thres]\n",
    "# print(len(tknzr_word.texts_to_sequences(all_data[column].values)))\n",
    "for w in tqdm(low_count_words):\n",
    "    del tknzr_word.word_index[w]\n",
    "    del tknzr_word.word_docs[w]\n",
    "    del tknzr_word.word_counts[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_word_seq = tknzr_word.texts_to_sequences(train[column].values)\n",
    "te_word_seq = tknzr_word.texts_to_sequences(test[column].values)\n",
    "config.maxlen = 800\n",
    "tr_word_pad = pad_sequences(tr_word_seq, maxlen=config.maxlen)\n",
    "te_word_pad = pad_sequences(te_word_seq, maxlen=config.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "max_features = 500000\n",
    "vec_len = 600\n",
    "EMBEDDING = '../feature/word2vec_file/word_and_article600d.w2v'\n",
    "model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "word_index = tknzr_word.word_index\n",
    "nb_words_desc = min(max_features, len(word_index))\n",
    "embedding_matrix_desc = np.zeros((nb_words_desc, vec_len))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix_desc[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import InputSpec, Layer\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras.layers import Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        \n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "        \n",
    "        # return flattened output\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dense\n",
    "# from keras.layers import GlobalMaxPool1D, GlobalMaxPool2D, SpatialDropout1D, Dropout, BatchNormalization, Lambda\n",
    "# from keras.layers import concatenate, Flatten, add, dot, PReLU, merge, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Add, Multiply\n",
    "# from keras.layers import LSTM, Conv1D, GlobalMaxPool2D, Convolution2D, Conv2D, CuDNNGRU, CuDNNLSTM, GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras import optimizers, losses, activations\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers import *\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_model():\n",
    "    features_input = Input(shape=(features.shape[1],))\n",
    "\n",
    "    \n",
    "    inpword = Input(shape=(config.maxlen, ))\n",
    "    emb_word = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)\n",
    "\n",
    "    lDropout_titl = SpatialDropout1D(0.5)(emb_word)\n",
    "    title_layer = Bidirectional(CuDNNLSTM(128, return_sequences=True))(lDropout_titl)\n",
    "\n",
    "    title_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))(title_layer)\n",
    "\n",
    "    max_pool_til = GlobalMaxPooling1D()(title_layer)\n",
    "    att = Attention(config.maxlen)(title_layer)\n",
    "# AttentionWeightedAverage()\n",
    "\n",
    "    all_views = concatenate([max_pool_til, att, features_input], axis=1)\n",
    "    x = Dropout(0.2)(all_views)\n",
    "\n",
    "    x = PReLU()(Dense(128)(x))\n",
    "\n",
    "    x = Dense(20, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[inpword, features_input], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_y_train=np_utils.to_categorical(Y,num_classes=20)\n",
    "one_y_val=np_utils.to_categorical(Y,num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# pretrain=False\n",
    "# if pretrain == True and len(X_test)<38:\n",
    "#     X_test.append(test_x)\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    now_nfold=0\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((102277,20)),np.zeros((102277,20))\n",
    "#     train_pred, test_pred = np.zeros((train_p_s,1)),np.zeros((test_p_s,1))\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        # x,y\n",
    "        now_nfold+=1\n",
    "        print (\"now is {} fold\".format(now_nfold))\n",
    "#         curr_x1, curr_x2 = tr_word_pad[train_index], tr_article_pad[train_index]\n",
    "        curr_x1 = tr_word_pad[train_index]\n",
    "#         hold_out_x1,hold_out_x2 =tr_word_pad[test_index],tr_article_pad[test_index]\n",
    "        hold_out_x1=tr_word_pad[test_index]\n",
    "        curr_y, hold_out_y = one_y_train[train_index], one_y_train[test_index]\n",
    "#         curr_x,curr_y = X[train_index],one_y_train[train_index]\n",
    "#         hold_out_x,hold_out_y =X[test_index],one_y_val[test_index]\n",
    "#         curr_x,curr_y = [np.array(x)[train_index] for x in X],one_y_train[train_index]\n",
    "#         curr_other_x = train_x[train_index]\n",
    "#         hold_out_x,hold_out_y = [np.array(x)[test_index] for x in X],one_y_train[test_index]\n",
    "#         hold_out_other_x = train_x[test_index]\n",
    "        kfold_X_features = features[train_index]\n",
    "        kfold_X_valid_features = features[test_index]\n",
    "        # curr_x ,curr_y= [x[train_idx] for x in X]\n",
    "        # hold_out_x = [x[valid_idx] for x in X]\n",
    "        # model\n",
    "        config.batch_size = 32\n",
    "        epochs = 15\n",
    "\n",
    "        model = get_rnn_model()\n",
    "\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, save_best_only=True,verbose=1, monitor='val_f1',  mode='max')\n",
    "        early = EarlyStopping( monitor='val_f1',  mode='max', patience=2,)\n",
    "        callbacks_list = [checkpoint,early]\n",
    "        # train and pred\n",
    "#         if pretrain == False:\n",
    "        model.fit([curr_x1, kfold_X_features], curr_y, \n",
    "                  batch_size=config.batch_size, epochs=epochs, \n",
    "                    validation_data=([hold_out_x1,kfold_X_valid_features], hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "#         else:\n",
    "#             curr_x.append(curr_other_x)\n",
    "#             hold_out_x.append(hold_out_other_x)\n",
    "            \n",
    "#             model.fit(X_test, test_y,\n",
    "#             batch_size=config.batch_size, epochs=1,\n",
    "#             validation_data=(hold_out_x, hold_out_y),\n",
    "#             callbacks=callbacks_list)\n",
    "            \n",
    "            \n",
    "\n",
    "#             model.fit(curr_x,  curr_y, \n",
    "#                       batch_size=config.batch_size, epochs=epochs, \n",
    "#                       validation_data=\n",
    "#                       (hold_out_x, hold_out_y), \n",
    "#                       callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "       \n",
    "        y_test = model.predict([te_word_pad, test_features])\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict([hold_out_x1,kfold_X_valid_features])\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    try:\n",
    "        print('all eval',sqrt(mean_squared_error(Y,train_pred)))\n",
    "    finally:\n",
    "        return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "now is 1 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "92049/92049 [==============================] - 718s 8ms/step - loss: 1.4885 - f1: 0.4960 - val_loss: 1.0956 - val_f1: 0.6480\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.64795, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      " 5696/92049 [>.............................] - ETA: 10:33 - loss: 1.1243 - f1: 0.6496"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-31fd8d2a0079>\u001b[0m in \u001b[0;36mkf_train\u001b[0;34m(fold_cnt, rnd)\u001b[0m\n\u001b[1;32m     42\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhold_out_x1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkfold_X_valid_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold_out_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                   callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;31m#         else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#             curr_x.append(curr_other_x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import pickle\n",
    "model_time = time.time()\n",
    "print(\"start\")\n",
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print (train_pred.shape,test_pred.shape)\n",
    "print (\"[{}] finished nn model\".format((time.time()-model_time)/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
