{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../feature/2_level_stacking_pkl/2leve_lgb_stacking.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/2_level_stacking_pkl/2leve_lr_stacking.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/2_level_stacking_pkl/2leve_nn_stacking.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "file path ../feature/2_level_stacking_pkl/2leve_nn_stackingml.pkl\n",
      "(102277, 19) (102277, 19)\n",
      "(102277, 76)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_x,test_x = [],[]\n",
    "for feat in sorted(glob.glob('../feature/2_level_stacking_pkl/*')):\n",
    "    if 'fm2.pkl' in feat or 'ligbm' in  feat :\n",
    "        continue\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "    \n",
    "\n",
    "train = pd.read_csv(\"../input/new_data/train_set.csv\")\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "y=(train[\"class\"]-1).astype(int)\n",
    "from keras.utils import np_utils\n",
    "train_y=np_utils.to_categorical(y,num_classes=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def self_f1_score(data, y_hat):\n",
    "    y_true = data\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return f1_score(y_true, y_hat, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def simple_ens(model_name,k=3,rnd=233,lr=0.05,feature_fraction=0.9,bagging_fraction=0.9,\n",
    "               bag_frec=3,met='binary_logloss'):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred, val_pred = np.zeros((102277,20)), np.zeros((102277,20))\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_f1_l,all_val_f1_l = 0,0\n",
    "    \n",
    "    for i in range(19):\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_f1_l,train_f1_l = 0,0\n",
    "        fold_cnt = 0\n",
    "        for train_index, test_index in kf.split(train_x,train_y[:,i]):\n",
    "            # x,y\n",
    "            curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "            hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "            d_test = test_x\n",
    "            # share params\n",
    "            params = {\n",
    "                    'application': 'binary',\n",
    "                    #'num_leaves': 8,\n",
    "                    #'lambda_l1': 1,\n",
    "                    'lambda_l2': 1.0,\n",
    "                    'max_depth': 4,\n",
    "                    #'scale_pos_weight':0.9,\n",
    "                    'metric': met, # or auc\n",
    "                    'data_random_seed': 2,\n",
    "                    'learning_rate':lr,\n",
    "                    # 'bagging_fraction': bagging_fraction,\n",
    "                    # 'bagging_freq':bag_frec,\n",
    "                    'feature_fraction': feature_fraction,\n",
    "\n",
    "                    }\n",
    "            if met == 'auc':\n",
    "                s_round = 100\n",
    "            else:\n",
    "                s_round = 50\n",
    "            # train for each class\n",
    "            d_train = lgb.Dataset(curr_x, curr_y[:,i])\n",
    "            d_valid = lgb.Dataset(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [d_train, d_valid]\n",
    "            model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=watchlist,\n",
    "                      early_stopping_rounds=s_round,\n",
    "                      verbose_eval=None)\n",
    "            print(fold_cnt,'fold: ',end='')\n",
    "            fold_cnt += 1\n",
    "            train_pred = model.predict(curr_x)\n",
    "            val_pred[test_index, i] = model.predict(hold_out_x)\n",
    "            curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "            curr_val_loss = log_loss(hold_out_y[:,i],val_pred[test_index, i])\n",
    "\n",
    "            curr_train_f1 = self_f1_score(curr_y[:,i],train_pred)\n",
    "            curr_val_f1 = self_f1_score(hold_out_y[:,i],val_pred[test_index, i])\n",
    "\n",
    "            print('ls',curr_train_loss,curr_val_loss,'f1',curr_train_f1,curr_val_f1)\n",
    "            val_loss_l += curr_val_loss\n",
    "            train_loss_l += curr_train_loss\n",
    "            val_f1_l += curr_val_f1\n",
    "            train_f1_l += curr_train_f1\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg k fold\n",
    "        train_loss_l = train_loss_l/k\n",
    "        val_loss_l = val_loss_l/k\n",
    "        train_f1_l = train_f1_l/k\n",
    "        val_f1_l = val_f1_l/k\n",
    "        print('this class avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this class f1 train',train_f1_l,'f1 val',val_f1_l)\n",
    "        \n",
    "        \n",
    "        # avg 6 class\n",
    "        all_train_loss_l += train_loss_l/19\n",
    "        all_val_loss_l += val_loss_l/19\n",
    "        all_train_f1_l += train_f1_l/19\n",
    "        all_val_f1_l += val_f1_l/19\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all f1 avg',all_train_f1_l,all_val_f1_l)\n",
    "    print('=======================================================')\n",
    "    return val_pred, test_pred\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:186: UserWarning: Usage subset(sliced data) of np.ndarray is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  warnings.warn(\"Usage subset(sliced data) of np.ndarray is not recommended due to it will double the peak memory cost in LightGBM.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.08880606520405047 0.09576106762535988 f1 0.8051755615061622 0.803217730009248\n",
      "1 fold: ls 0.09427630743150099 0.09896217279710383 f1 0.7997127117335321 0.7953951939305917\n",
      "2 fold: ls 0.09419485651511902 0.10272405643904228 f1 0.8000089271470618 0.783342000393225\n",
      "3 fold: ls 0.09453740704211325 0.10042009086771904 f1 0.7987187793495825 0.7923182794174546\n",
      "4 fold: ls 0.09541127787180824 0.09694931907573348 f1 0.7989445893813214 0.8005089720251142\n",
      "5 fold: ls 0.09180329083480955 0.10118744758618112 f1 0.8019971840889325 0.8022244875970728\n",
      "6 fold: ls 0.09456499194000459 0.1035045996344501 f1 0.800647618014901 0.7883464376920093\n",
      "7 fold: ls 0.09225123750416525 0.09868609124353649 f1 0.8031472541068991 0.7973597732967104\n",
      "8 fold: ls 0.09363118082803158 0.09995984128400906 f1 0.8002055755580528 0.7859364865202989\n",
      "9 fold: ls 0.0947756895780983 0.09957096728458081 f1 0.7999675557366135 0.7928527503958323\n",
      "this class avg train 0.09342523047497013 avg val 0.0997725653837716\n",
      "this class f1 train 0.8008525756623058 f1 val 0.7941502111277557\n",
      "========================\n",
      "0 fold: ls 0.027503340391085512 0.03334504885133484 f1 0.9032746115752108 0.8957623865252511\n",
      "1 fold: ls 0.027962511143037494 0.02960666295308969 f1 0.9041950480122294 0.906798925858978\n",
      "2 fold: ls 0.027262070112001182 0.034040358649096304 f1 0.904909833429761 0.888879398189774\n",
      "3 fold: ls 0.02746098517788246 0.03493276264305661 f1 0.905809622986897 0.894617692780449\n",
      "4 fold: ls 0.0270093388674327 0.03066834289644876 f1 0.9033725523263458 0.9114067683565903\n",
      "5 fold: ls 0.027474696844312415 0.032581127225675376 f1 0.9040417897184583 0.8892339635175162\n",
      "6 fold: ls 0.027098411007157343 0.0359072563802482 f1 0.9061714605791935 0.8873073028707297\n",
      "7 fold: ls 0.02725094537829228 0.03274107253893336 f1 0.9072302242298975 0.8903969811831597\n",
      "8 fold: ls 0.027422461176447678 0.030693695327178614 f1 0.9029388350213903 0.9035052494201068\n",
      "9 fold: ls 0.02786639547130255 0.03176728665432383 f1 0.9045659066295948 0.8886816755256427\n",
      "this class avg train 0.027431115556895164 avg val 0.03262836141193856\n",
      "this class f1 train 0.9046509884508976 f1 val 0.8956590344228197\n",
      "========================\n",
      "0 fold: ls 0.0397248974666999 0.043584915052640005 f1 0.9550601946741903 0.9546240327507458\n",
      "1 fold: ls 0.03734478192132054 0.04621969033104149 f1 0.9565782755232766 0.9504200419671024\n",
      "2 fold: ls 0.03980101590582954 0.049617402833574194 f1 0.9552664471997264 0.9488610581262841\n",
      "3 fold: ls 0.038638301234124824 0.04761163499004716 f1 0.9555891197053196 0.9524973158961696\n",
      "4 fold: ls 0.038563646526178164 0.04040403012593658 f1 0.9561852723358124 0.9525635839532391\n",
      "5 fold: ls 0.036909818678950816 0.04461544914835986 f1 0.9573449468351063 0.9518953940063353\n",
      "6 fold: ls 0.03974024302417611 0.042089403559764364 f1 0.9556370761268738 0.9516781504317666\n",
      "7 fold: ls 0.03576160890642927 0.03965585578118495 f1 0.9577191139681318 0.9583895184273545\n",
      "8 fold: ls 0.03910767171096004 0.04587516400298288 f1 0.9554572625243982 0.95061609900977\n",
      "9 fold: ls 0.03914202768344254 0.042647532393494225 f1 0.9554628803010663 0.9541049019674963\n",
      "this class avg train 0.03847340130581117 avg val 0.04423210782190257\n",
      "this class f1 train 0.9560300589193902 f1 val 0.9525650096536264\n",
      "========================\n",
      "0 fold: ls 0.0226746848508282 0.031877896527468724 f1 0.9425427944489528 0.9315760849845712\n",
      "1 fold: ls 0.024270237543489937 0.025904857231575318 f1 0.9398526895959725 0.9350622143219909\n",
      "2 fold: ls 0.024920353538950123 0.027926077953370373 f1 0.9368254847482638 0.9326981280336271\n",
      "3 fold: ls 0.02444537841070995 0.032972910950246036 f1 0.9378729267651752 0.9236863496457446\n",
      "4 fold: ls 0.024507828210221287 0.02902772048366058 f1 0.9392022598080136 0.9307254576584261\n",
      "5 fold: ls 0.024196999062184428 0.026985326795393477 f1 0.9395293198140724 0.9325247786866182\n",
      "6 fold: ls 0.024008928175607083 0.028134423746789712 f1 0.9391524867746573 0.9352245643917658\n",
      "7 fold: ls 0.018718568877641895 0.026452662642999 f1 0.9532186106509394 0.9344433806320682\n",
      "8 fold: ls 0.022302050502464456 0.028202907207010984 f1 0.9431717113108982 0.9318440999262662\n",
      "9 fold: ls 0.023620044296208495 0.0330588653400977 f1 0.9408218002485018 0.9188905139671649\n",
      "this class avg train 0.023366507346830583 avg val 0.029054364887861193\n",
      "this class f1 train 0.9412190084165447 f1 val 0.9306675572248242\n",
      "========================\n",
      "0 fold: ls 0.018936373464226816 0.021118109438150207 f1 0.9304357930879096 0.9240460394879108\n",
      "1 fold: ls 0.018251974390599626 0.021068185335051278 f1 0.9335630152618368 0.9191610330593646\n",
      "2 fold: ls 0.018701983416090983 0.029149711417320225 f1 0.9332089172303144 0.9004553071686547\n",
      "3 fold: ls 0.017177187329954602 0.020525565703831378 f1 0.9368958973231237 0.9256564683470216\n",
      "4 fold: ls 0.017859557813669078 0.021420778435522085 f1 0.932591136190843 0.9262974332367735\n",
      "5 fold: ls 0.018326847717121227 0.023430183168371664 f1 0.933005292752373 0.923410302141592\n",
      "6 fold: ls 0.018733647326733806 0.026325048463930238 f1 0.9317352344785762 0.9090106575315093\n",
      "7 fold: ls 0.01897922749185996 0.023287109461828312 f1 0.931931394384522 0.9141967155639795\n",
      "8 fold: ls 0.018962554583018654 0.02232418760299749 f1 0.9310000738341081 0.9240458720548901\n",
      "9 fold: ls 0.018812650616649697 0.023396087309795935 f1 0.9323474677639212 0.9106066386362118\n",
      "this class avg train 0.018474200414992446 avg val 0.023204496633679884\n",
      "this class f1 train 0.9326714222307528 f1 val 0.9176886467227907\n",
      "========================\n",
      "0 fold: ls 0.02680094290036061 0.026561937624292067 f1 0.9667744381794361 0.9667820749897889\n",
      "1 fold: ls 0.02725039178324624 0.031127655596062693 f1 0.9658602073080922 0.9621462027050081\n",
      "2 fold: ls 0.027121940219859534 0.031297536233589995 f1 0.9660943421823083 0.9636184630840545\n",
      "3 fold: ls 0.027649425862772994 0.033043786383832596 f1 0.9660372217308653 0.962250711669618\n",
      "4 fold: ls 0.02697739472978104 0.03270345652642915 f1 0.9660007099075695 0.9644877944495063\n",
      "5 fold: ls 0.024708664457723854 0.032784736159141986 f1 0.9680588566774468 0.9656663879472521\n",
      "6 fold: ls 0.027267671867530152 0.03312346491755795 f1 0.9660372217308653 0.9627885076591703\n",
      "7 fold: ls 0.026040045000636666 0.03797074788955996 f1 0.9669458700175539 0.9560191450604225\n",
      "8 fold: ls 0.02837577457482403 0.030853663895678143 f1 0.9660163942207327 0.9654770009054173\n",
      "9 fold: ls 0.027699954342228147 0.03560905260196228 f1 0.9665832201743318 0.959607949440813\n",
      "this class avg train 0.026989220573896328 avg val 0.03250760378281068\n",
      "this class f1 train 0.9664408482129202 f1 val 0.962884423791105\n",
      "========================\n",
      "0 fold: ls 0.03974002919723301 0.050856847087349645 f1 0.8647805246104547 0.828994666532912\n",
      "1 fold: ls 0.03986338668591424 0.04547359044668923 f1 0.8648103286923664 0.8578574215992235\n",
      "2 fold: ls 0.03976586173801365 0.044883692997871726 f1 0.8637530584808557 0.8580405528956301\n",
      "3 fold: ls 0.0394804266806385 0.04875500606849514 f1 0.8680442826377557 0.8310455754064776\n",
      "4 fold: ls 0.04104257612787043 0.04041827470590256 f1 0.8616643961907098 0.8626338670537566\n",
      "5 fold: ls 0.04057219759163977 0.04774644252787868 f1 0.8641205536363995 0.848225950894007\n",
      "6 fold: ls 0.041568499865568945 0.046799235002568114 f1 0.8597829119558317 0.8514303335975941\n",
      "7 fold: ls 0.03947778423904392 0.04522042256770816 f1 0.8652409615520789 0.8513984720886738\n",
      "8 fold: ls 0.040256633075205224 0.04361121653111919 f1 0.8634457828759226 0.8557905442239253\n",
      "9 fold: ls 0.04080983493874488 0.047342123996130615 f1 0.8612169905278724 0.852246110979297\n",
      "this class avg train 0.04025772301398726 avg val 0.04611068519317131\n",
      "this class f1 train 0.8636859791160247 f1 val 0.8497663495271498\n",
      "========================\n",
      "0 fold: ls 0.07509616801236134 0.07501752155828709 f1 0.8739458127372208 0.8845281144521416\n",
      "1 fold: ls 0.07692681505815191 0.07652584685472756 f1 0.8738822239322499 0.8759980579855742\n",
      "2 fold: ls 0.07665379215600918 0.08557584645881301 f1 0.874704297561795 0.8632863131098414\n",
      "3 fold: ls 0.07575862857723491 0.08143228135779679 f1 0.8749712561662633 0.8710349037056075\n",
      "4 fold: ls 0.07687396753573736 0.0829506991380488 f1 0.8743524269885281 0.8674945542895949\n",
      "5 fold: ls 0.07675515023487109 0.08126398935280475 f1 0.8744827141483036 0.8664324521993523\n",
      "6 fold: ls 0.07708563280671943 0.08229129566976576 f1 0.8736449428654249 0.8765616823652538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 fold: ls 0.0756174323713398 0.08393770711074289 f1 0.8744644199346419 0.8656691601659485\n",
      "8 fold: ls 0.07432193056315904 0.0883884360400981 f1 0.8769112079088492 0.8571745470362013\n",
      "9 fold: ls 0.07638429487886236 0.0816082520313408 f1 0.8753092104505411 0.8656355607459582\n",
      "this class avg train 0.07614738121944464 avg val 0.08189918755724254\n",
      "this class f1 train 0.8746668512693816 f1 val 0.8693815346055473\n",
      "========================\n",
      "0 fold: ls 0.023294266569339 0.026566737075096186 f1 0.9729754554077596 0.9701991602831441\n",
      "1 fold: ls 0.02322344057811677 0.026470469421864845 f1 0.9735823242383443 0.9704677263073691\n",
      "2 fold: ls 0.023390420846695403 0.02580229346742035 f1 0.9733957753520324 0.9688795900032308\n",
      "3 fold: ls 0.02077548003578063 0.02210651866323882 f1 0.9751269063613491 0.9778069142463892\n",
      "4 fold: ls 0.023087994800617705 0.02909363149097814 f1 0.9734446207950967 0.9654253637194776\n",
      "5 fold: ls 0.02282866061616694 0.03596358095041251 f1 0.9739992833111033 0.9649630979282262\n",
      "6 fold: ls 0.022759191629129448 0.02623390849996695 f1 0.9734302104939316 0.9712582463567643\n",
      "7 fold: ls 0.02342048235755284 0.025007010813735412 f1 0.9729901929237139 0.9725546164627551\n",
      "8 fold: ls 0.02339793164505228 0.03369232478562153 f1 0.9736598047028213 0.9682673153658552\n",
      "9 fold: ls 0.023658375533708646 0.02861547588632634 f1 0.9727131846901698 0.97313691959995\n",
      "this class avg train 0.02298362446121597 avg val 0.027955195105466107\n",
      "this class f1 train 0.9735317758276322 f1 val 0.9702958950273162\n",
      "========================\n",
      "0 fold: ls 0.06328421262359417 0.06886069344662044 f1 0.8597489646256469 0.8452436114212217\n",
      "1 fold: ls 0.06264974411264031 0.07176864291916336 f1 0.8588642266178725 0.853785740214531\n",
      "2 fold: ls 0.06438910349440302 0.06899910676435105 f1 0.8572158932743863 0.8487399040932871\n",
      "3 fold: ls 0.06486855056437034 0.06749842575772987 f1 0.857317511112219 0.8486636648716932\n",
      "4 fold: ls 0.06369518629372645 0.07245181691390973 f1 0.8596082264283755 0.8494428575707245\n",
      "5 fold: ls 0.06488365926422303 0.07034986592308988 f1 0.8564514309308624 0.8496371459617413\n",
      "6 fold: ls 0.06455487451895549 0.07471631867347563 f1 0.8577503266717832 0.8563650250387371\n",
      "7 fold: ls 0.06173277526808947 0.07021407480140729 f1 0.862660093966941 0.8467197678040417\n",
      "8 fold: ls 0.06417374804455363 0.06998260933496157 f1 0.8589879043267581 0.8488655394127717\n",
      "9 fold: ls 0.06499758124050968 0.06770238648300801 f1 0.8574833492047663 0.8448852827335855\n",
      "this class avg train 0.06392294354250656 avg val 0.07025439410177167\n",
      "this class f1 train 0.858608792715961 f1 val 0.8492348539122336\n",
      "========================\n",
      "0 fold: ls 0.04306669278482496 0.050045525450127824 f1 0.8686665861321456 0.8511684971251505\n",
      "1 fold: ls 0.042825111455733374 0.053300137152985566 f1 0.8689292364417779 0.8452506006268133\n",
      "2 fold: ls 0.04409294196939703 0.051546322859933554 f1 0.8679445170414848 0.8480439471046763\n",
      "3 fold: ls 0.04394097397749381 0.046743806132655996 f1 0.866957404296604 0.8505060719641143\n",
      "4 fold: ls 0.04334222076150184 0.0513879116169176 f1 0.8679166269890158 0.8566350722354068\n",
      "5 fold: ls 0.043077920197185975 0.04897002875358338 f1 0.8690135131316676 0.8580001403856572\n",
      "6 fold: ls 0.043907142513781465 0.053011636202011374 f1 0.8669798687328749 0.858282507519943\n",
      "7 fold: ls 0.044087161245272066 0.047915806419264656 f1 0.865743673394721 0.8617605281482459\n",
      "8 fold: ls 0.043339208352473596 0.04332917433136904 f1 0.8659917303345048 0.8855882986216048\n",
      "9 fold: ls 0.04308656312508571 0.04580203210491652 f1 0.8659904758095818 0.8721888711569638\n",
      "this class avg train 0.04347659363827498 avg val 0.04920523810237655\n",
      "this class f1 train 0.8674133632304379 f1 val 0.8587424534888577\n",
      "========================\n",
      "0 fold: ls 0.06967418349798403 0.07634020787701223 f1 0.851354643437177 0.846168816836656\n",
      "1 fold: ls 0.07075387998184843 0.0735419604325597 f1 0.8500182875498099 0.8518236882063259\n",
      "2 fold: ls 0.07075086168411078 0.07278864124377024 f1 0.8513699731552204 0.8453893952698412\n",
      "3 fold: ls 0.07100640761692198 0.07522328354461744 f1 0.8489188795063902 0.8536708857123555\n",
      "4 fold: ls 0.07090104295905836 0.07613365217626455 f1 0.8503034677175851 0.8408399433527809\n",
      "5 fold: ls 0.07066468370146593 0.07802850866291852 f1 0.8496859087248521 0.843336896887882\n",
      "6 fold: ls 0.07110809678239836 0.07702816030384144 f1 0.8500525786801754 0.8405117799506805\n",
      "7 fold: ls 0.06877993089523615 0.08040986780437905 f1 0.8528962286598505 0.8305001914940793\n",
      "8 fold: ls 0.07194540590179505 0.0773837818789603 f1 0.849031106285695 0.8465751142099137\n",
      "9 fold: ls 0.06942560370823579 0.07720639522096338 f1 0.8506138382384967 0.8462495149171887\n",
      "this class avg train 0.07050100967290548 avg val 0.0764084459145287\n",
      "this class f1 train 0.8504244911955252 f1 val 0.8445066226837703\n",
      "========================\n",
      "0 fold: ls 0.07374103507493872 0.08026141731007434 f1 0.8956486773979848 0.8875082642794105\n",
      "1 fold: ls 0.0745903215997996 0.07693051064347009 f1 0.8950067638421897 0.8954872353525443\n",
      "2 fold: ls 0.07433389938851631 0.08269218219965853 f1 0.8956038353973148 0.8805762115776886\n",
      "3 fold: ls 0.07515035253159223 0.08155270038291702 f1 0.8948493223586371 0.8897613361437623\n",
      "4 fold: ls 0.07460290110249405 0.0818410066661251 f1 0.8953210412625012 0.8949182688514242\n",
      "5 fold: ls 0.07543452349512188 0.08295696086378047 f1 0.895325324733206 0.8873690450236942\n",
      "6 fold: ls 0.07497472126867649 0.08110669786249913 f1 0.8951687607985696 0.88879951123271\n",
      "7 fold: ls 0.0745851375649133 0.07761799390513648 f1 0.8953184539250696 0.8975446925005026\n",
      "8 fold: ls 0.07386884967607792 0.0753691581755668 f1 0.8952629236276235 0.9018635124005234\n",
      "9 fold: ls 0.07215597786788097 0.08238084267692952 f1 0.8988184448272709 0.8838692155489445\n",
      "this class avg train 0.07434377195700115 avg val 0.08027094706861575\n",
      "this class f1 train 0.8956323548170367 f1 val 0.8907697292911205\n",
      "========================\n",
      "0 fold: ls 0.05462741429908799 0.06487509806866663 f1 0.9128685133303881 0.9006161460937727\n",
      "1 fold: ls 0.05565484135459886 0.058614934494318906 f1 0.9116378515571688 0.9112343120867944\n",
      "2 fold: ls 0.05557367259982226 0.053257376063304315 f1 0.9118560928321062 0.9134754450006745\n",
      "3 fold: ls 0.05548637035605078 0.061464095439115254 f1 0.9115931867029023 0.905912747989392\n",
      "4 fold: ls 0.054713803773691075 0.06265103234860953 f1 0.9134656456834762 0.8994676847622456\n",
      "5 fold: ls 0.05641667225326139 0.05809081376275264 f1 0.9122716934324469 0.9123579013997161\n",
      "6 fold: ls 0.053904179366939114 0.06201323796914104 f1 0.9136664465654545 0.9052318787498256\n",
      "7 fold: ls 0.055437677972119834 0.06213182129307742 f1 0.9119337572266906 0.9063796853404193\n",
      "8 fold: ls 0.05557479383777512 0.06674098934116739 f1 0.9117757716515675 0.904772469638109\n",
      "9 fold: ls 0.05546158075546686 0.05789590661878253 f1 0.9111756953654492 0.9145668112399017\n",
      "this class avg train 0.05528510065688134 avg val 0.06077353053989356\n",
      "this class f1 train 0.912224465434765 f1 val 0.907401508230085\n",
      "========================\n",
      "0 fold: ls 0.030983354375617927 0.03613112589988497 f1 0.9609962118421369 0.9571330693128943\n",
      "1 fold: ls 0.030911913179566257 0.03141427505719661 f1 0.9611378059548288 0.9614963525436038\n",
      "2 fold: ls 0.030512861685624185 0.03629404001670292 f1 0.962459851117998 0.9514117507337847\n",
      "3 fold: ls 0.03222540479945519 0.03148083592218271 f1 0.9603649670271046 0.9616945778975201\n",
      "4 fold: ls 0.031710764162697566 0.03996565005093261 f1 0.9610398637819594 0.9542771663458915\n",
      "5 fold: ls 0.03106690361492179 0.039093153762165975 f1 0.9616443421736494 0.9515649924317611\n",
      "6 fold: ls 0.03173798353534735 0.03567194859171336 f1 0.9606864706600304 0.9590384252123634\n",
      "7 fold: ls 0.031106573256520796 0.03842944671701978 f1 0.9605894113331408 0.9596371885665933\n",
      "8 fold: ls 0.032106421472575454 0.031472635692735625 f1 0.9606429394872973 0.9648625929370158\n",
      "9 fold: ls 0.03104593948348348 0.039960239905789154 f1 0.9615133081545595 0.9513499498606393\n",
      "this class avg train 0.031340811956581005 avg val 0.03599133516163237\n",
      "this class f1 train 0.9611075171532704 f1 val 0.9572466065842067\n",
      "========================\n",
      "0 fold: ls 0.019383240979898225 0.026788651754656057 f1 0.9353170171730513 0.9096465270112778\n",
      "1 fold: ls 0.021461542322238833 0.030596841086190252 f1 0.9285396384048867 0.9110148724219767\n",
      "2 fold: ls 0.021638706918761703 0.029360199027241612 f1 0.9292678968670025 0.9105934049487352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 fold: ls 0.021575531585384083 0.02408357974569117 f1 0.928411911739478 0.9267518771960034\n",
      "4 fold: ls 0.02065684697851248 0.025973123092035956 f1 0.9317796112880041 0.9073252312952187\n",
      "5 fold: ls 0.02124120711040182 0.02414602291180266 f1 0.9292501312779567 0.9248947701801167\n",
      "6 fold: ls 0.021984951375978317 0.026250473056379914 f1 0.9276675765582919 0.9107278603170086\n",
      "7 fold: ls 0.021580945005048396 0.02658783874055033 f1 0.928116279844902 0.9180496544112356\n",
      "8 fold: ls 0.0203423485295453 0.02287177734979841 f1 0.9311356292354694 0.9290293479358029\n",
      "9 fold: ls 0.021959891294289133 0.026860113821662413 f1 0.9287888523834409 0.9059722076284544\n",
      "this class avg train 0.021182521210005828 avg val 0.026351862058600878\n",
      "this class f1 train 0.9298274544772485 f1 val 0.915400575334583\n",
      "========================\n",
      "0 fold: ls 0.032665216804624614 0.03923999458949494 f1 0.8913383731764049 0.8830370873657696\n",
      "1 fold: ls 0.03230475927772502 0.03309626070819445 f1 0.8915833314781665 0.8882099774139494\n",
      "2 fold: ls 0.031580607661532374 0.03896182126330606 f1 0.893888426908124 0.8598995072918316\n",
      "3 fold: ls 0.03155011592971975 0.03838819117126557 f1 0.8918146609445248 0.8848870236378121\n",
      "4 fold: ls 0.03207282396555688 0.0414883302075925 f1 0.892294384554885 0.8801393992457825\n",
      "5 fold: ls 0.03154827169320123 0.03900120839530688 f1 0.8919342878167547 0.87797107674685\n",
      "6 fold: ls 0.03150458113433673 0.03319395385749963 f1 0.8903577424759972 0.8988969351533298\n",
      "7 fold: ls 0.03170127863319293 0.03400161700556483 f1 0.8916969021563348 0.8999108192443703\n",
      "8 fold: ls 0.03204029184700642 0.034036267012151816 f1 0.8918944540271571 0.8890333343505549\n",
      "9 fold: ls 0.031959322272234476 0.03679007802101557 f1 0.8922138365756349 0.8796197132294286\n",
      "this class avg train 0.03189272692191304 avg val 0.03681977222313923\n",
      "this class f1 train 0.8919016400113986 f1 val 0.8841604873679678\n",
      "========================\n",
      "0 fold: ls 0.03930243195371197 0.04226013540296702 f1 0.944951920518961 0.9479407156688398\n",
      "1 fold: ls 0.04003355505962537 0.044885916585051415 f1 0.9446321222338946 0.9378922274199353\n",
      "2 fold: ls 0.04022218025962486 0.04538841868938438 f1 0.9443611420235413 0.9461752179344259\n",
      "3 fold: ls 0.04040811596542654 0.045599592381886545 f1 0.9444964185821135 0.9378226822674451\n",
      "4 fold: ls 0.03948378216470035 0.04206281435712235 f1 0.9452330788989757 0.9446263033426108\n",
      "5 fold: ls 0.04039444414985554 0.04647329658694996 f1 0.9456963506563407 0.9373202800890978\n",
      "6 fold: ls 0.04021568980256038 0.04659190189197333 f1 0.9447467825118131 0.9383936788080482\n",
      "7 fold: ls 0.039844521927265265 0.04576771943256525 f1 0.9447401091689525 0.9407334872681407\n",
      "8 fold: ls 0.04036472069880971 0.04294494182181705 f1 0.9453712047527991 0.9393229160544733\n",
      "9 fold: ls 0.04062101582019705 0.046514220159869606 f1 0.9447033036112247 0.9406741402802247\n",
      "this class avg train 0.040089045780177704 avg val 0.044848895730958696\n",
      "this class f1 train 0.9448932432958614 f1 val 0.9410901649133241\n",
      "========================\n",
      "0 fold: ls 0.0893537586155754 0.08975904972251236 f1 0.8131532506295436 0.8277470744993523\n",
      "1 fold: ls 0.08915983368283908 0.10177700739219256 f1 0.8167025876296448 0.797566951723854\n",
      "2 fold: ls 0.08947339438732485 0.09298250967793442 f1 0.8145240526520913 0.8145174716452637\n",
      "3 fold: ls 0.08903302762282683 0.09242469733506982 f1 0.8150097280765974 0.8095430812505318\n",
      "4 fold: ls 0.08855701974198843 0.09820024010283825 f1 0.8170658724938055 0.8075711948509352\n",
      "5 fold: ls 0.08783344364465627 0.09593600693251748 f1 0.8169052212943171 0.8064879858087404\n",
      "6 fold: ls 0.0896612076698593 0.09287431018407534 f1 0.8166622132354768 0.7991810957629796\n",
      "7 fold: ls 0.08869791343612213 0.09113901280519585 f1 0.8153709563394931 0.8145968280619637\n",
      "8 fold: ls 0.08746138721881326 0.09695497960587553 f1 0.8158563637886234 0.8043593541602545\n",
      "9 fold: ls 0.08887475974154135 0.09457440867649365 f1 0.8143578953352547 0.81564994137842\n",
      "this class avg train 0.08881057457615468 avg val 0.09466222224347053\n",
      "this class f1 train 0.8155608141474847 f1 val 0.8097220979142294\n",
      "========================\n",
      "all loss avg 0.04675755285686554 0.05226059004857013\n",
      "all f1 avg 0.902175981293939 0.8948070400959639\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "lgb_train, lgb_res = simple_ens('lgb',10,233,0.05,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
