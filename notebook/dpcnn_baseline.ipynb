{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from collections import Counter\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Input, Embedding, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import optimizers, losses, activations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from models_def import Attention\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "config = argparse.Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "der=True\n",
    "if der:\n",
    "    train_dir = '../input/new_data/train_set.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "else:\n",
    "#     train_dir = '../input/new_data/train_remove60.csv'\n",
    "#     test_dir = '../input/new_data/test_remove60.csv'\n",
    "    train_dir = '../input/new_data/train_pre_enhance.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "train = pd.read_csv(train_dir)\n",
    "test = pd.read_csv(test_dir)\n",
    "original_train = pd.read_csv('../input/new_data/train_set.csv')\n",
    "shuffle_data = train[original_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "train_offset = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "df_y_train = (train[\"class\"]-1).astype(int)\n",
    "# column = \"word_seg\"\n",
    "test_id = test[[\"id\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"word_seg\"\n",
    "config.len_desc = 800000\n",
    "tknzr_word = Tokenizer(num_words=config.len_desc)\n",
    "tknzr_word.fit_on_texts(all_data[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912182/912182 [00:01<00:00, 749452.81it/s]\n"
     ]
    }
   ],
   "source": [
    "count_thres = 5\n",
    "low_count_words = [w for w, c in tknzr_word.word_counts.items() if c < count_thres]\n",
    "for w in tqdm(low_count_words):\n",
    "    del tknzr_word.word_index[w]\n",
    "    del tknzr_word.word_docs[w]\n",
    "    del tknzr_word.word_counts[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_word_seq = tknzr_word.texts_to_sequences(train[column].values)\n",
    "te_word_seq = tknzr_word.texts_to_sequences(test[column].values)\n",
    "config.maxlen = 300\n",
    "tr_word_pad = pad_sequences(tr_word_seq, maxlen=config.maxlen)\n",
    "te_word_pad = pad_sequences(te_word_seq, maxlen=config.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (102277, 300)\n"
     ]
    }
   ],
   "source": [
    "tr_word_post = pad_sequences(tr_word_seq, maxlen=config.maxlen, padding='post', truncating='post')\n",
    "print('Shape of data tensor:', tr_word_post.shape)\n",
    "# print('Shape of label tensor:', y.shape)\n",
    "\n",
    "te_word_post = pad_sequences(te_word_pad, maxlen=config.maxlen, padding='post', truncating='post')\n",
    "# print('Shape of test_data tensor:', test_data_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "max_features = 800000\n",
    "vec_len = 600\n",
    "EMBEDDING = '../feature/word2vec_file/avito600d.w2v'\n",
    "model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "word_index = tknzr_word.word_index\n",
    "nb_words_desc = min(max_features, len(word_index))\n",
    "embedding_matrix_desc = np.zeros((nb_words_desc+1, vec_len))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix_desc[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column = \"article\"\n",
    "# config.len_title = 100000\n",
    "# tknzr_article = Tokenizer(num_words=config.len_title)\n",
    "# tknzr_article.fit_on_texts(all_data[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_thres = 4\n",
    "# low_count_words = [w for w, c in tknzr_article.word_counts.items() if c < count_thres]\n",
    "# # print(len(tknzr_word.texts_to_sequences(all_data[column].values)))\n",
    "# for w in tqdm(low_count_words):\n",
    "#     del tknzr_article.word_index[w]\n",
    "#     del tknzr_article.word_docs[w]\n",
    "#     del tknzr_article.word_counts[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_article_seq = tknzr_article.texts_to_sequences(train[column].values)\n",
    "# te_article_seq = tknzr_article.texts_to_sequences(test[column].values)\n",
    "# config.titlemaxlen= 300\n",
    "# tr_article_pad = pad_sequences(tr_article_seq, maxlen=config.titlemaxlen)\n",
    "# te_article_pad = pad_sequences(te_article_seq, maxlen=config.titlemaxlen)\n",
    "# del all_data\n",
    "# gc.collect()\n",
    "# article_index = tknzr_article.word_index\n",
    "# nb_article = min(max_features, len(article_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features = 500000\n",
    "# vec_len = 600\n",
    "# EMBEDDING = \"../feature/word2vec_file/avito600d.w2v\"\n",
    "# model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "# char_embedding_matrix = np.zeros((nb_article+1, vec_len))\n",
    "# for word, i in article_index.items():\n",
    "#     if i >= max_features: continue\n",
    "#     try:\n",
    "#         embedding_vector = model[word]\n",
    "#     except KeyError:\n",
    "#         embedding_vector = None\n",
    "#     if embedding_vector is not None: char_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "from keras.engine import Layer, InputSpec\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    filter_nr = 128\n",
    "    filter_size = 3\n",
    "    max_pool_size = 3\n",
    "    max_pool_strides = 2\n",
    "    dense_nr = 256\n",
    "    spatial_dropout = 0.4\n",
    "    dense_dropout = 0.4\n",
    "    \n",
    "    inpword = Input(shape=(1000, ))\n",
    "    inparticle = Input(shape=(1000, ))\n",
    "    \n",
    "\n",
    "    emb_comment1 = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)\n",
    "#     emb_comment1 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inpword)\n",
    "    emb_comment1 = SpatialDropout1D(spatial_dropout)(emb_comment1)\n",
    "\n",
    "    block11 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_comment1)\n",
    "    block11 = BatchNormalization()(block11)\n",
    "    block11 = PReLU()(block11)\n",
    "    block11 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block11)\n",
    "    block11 = BatchNormalization()(block11)\n",
    "    block11 = PReLU()(block11)\n",
    "\n",
    "    resize_emb1 = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_comment1)\n",
    "    resize_emb1 = PReLU()(resize_emb1)\n",
    "\n",
    "    block1_output1 = add([block11, resize_emb1])\n",
    "    block1_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output1)\n",
    "\n",
    "    block21 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output1)\n",
    "    block21 = BatchNormalization()(block21)\n",
    "    block21 = PReLU()(block21)\n",
    "    block21 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block21)\n",
    "    block21 = BatchNormalization()(block21)\n",
    "    block21 = PReLU()(block21)\n",
    "\n",
    "    block2_output1 = add([block21, block1_output1])\n",
    "    block2_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output1)\n",
    "\n",
    "    block31 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output1)\n",
    "    block31 = BatchNormalization()(block31)\n",
    "    block31 = PReLU()(block31)\n",
    "    block31 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block31)\n",
    "    block31 = BatchNormalization()(block31)\n",
    "    block31 = PReLU()(block31)\n",
    "\n",
    "    block3_output1 = add([block31, block2_output1])\n",
    "    block3_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output1)\n",
    "\n",
    "    block41 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output1)\n",
    "    block41 = BatchNormalization()(block41)\n",
    "    block41 = PReLU()(block41)\n",
    "    block41 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block41)\n",
    "    block41 = BatchNormalization()(block41)\n",
    "    block41 = PReLU()(block41)\n",
    "\n",
    "    output1 = add([block41, block3_output1])\n",
    "    output1 =GlobalMaxPooling1D()(output1)\n",
    "\n",
    "\n",
    "    emb_comment2 =  Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inparticle)\n",
    "    emb_comment2 = SpatialDropout1D(spatial_dropout)(emb_comment2)\n",
    "\n",
    "    block12 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_comment2)\n",
    "    block12 = BatchNormalization()(block12)\n",
    "    block12 = PReLU()(block12)\n",
    "    block12 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block12)\n",
    "    block12 = BatchNormalization()(block12)\n",
    "    block12 = PReLU()(block12)\n",
    "\n",
    "    resize_emb2 = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_comment2)\n",
    "    resize_emb2 = PReLU()(resize_emb2)\n",
    "\n",
    "    block1_output2 = add([block12, resize_emb2])\n",
    "    block1_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output2)\n",
    "\n",
    "    block22 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output2)\n",
    "    block22 = BatchNormalization()(block22)\n",
    "    block22 = PReLU()(block22)\n",
    "    block22 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block22)\n",
    "    block22 = BatchNormalization()(block22)\n",
    "    block22 = PReLU()(block22)\n",
    "\n",
    "    block2_output2 = add([block22, block1_output2])\n",
    "    block2_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output2)\n",
    "\n",
    "    block32 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output2)\n",
    "    block32 = BatchNormalization()(block32)\n",
    "    block32 = PReLU()(block32)\n",
    "    block32 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block32)\n",
    "    block32 = BatchNormalization()(block32)\n",
    "    block32 = PReLU()(block32)\n",
    "\n",
    "    block3_output2 = add([block32, block2_output2])\n",
    "    block3_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output2)\n",
    "\n",
    "    block42 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output2)\n",
    "    block42 = BatchNormalization()(block42)\n",
    "    block42 = PReLU()(block42)\n",
    "    block42 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block42)\n",
    "    block42 = BatchNormalization()(block42)\n",
    "    block42 = PReLU()(block42)\n",
    "\n",
    "    output2 = add([block42, block3_output2])\n",
    "    output2 = GlobalMaxPooling1D()(output2)\n",
    "\n",
    "            \n",
    "    output = concatenate([output1, output2])\n",
    "    output = Dense(dense_nr, activation='linear')(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    x = Dense(19, activation=\"softmax\")(output)\n",
    "    model = Model(inputs=[inpword, inparticle], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_Capsule_model():\n",
    "#     features_input = Input(shape=(features.shape[1],))\n",
    "\n",
    "    inpword = Input(shape=(config.maxlen, ))\n",
    "    emb_word = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)\n",
    "    embed_layer = SpatialDropout1D(0.4)(emb_word)\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(embed_layer)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    capsule = Capsule(num_capsule=10, dim_capsule=32, routings=5,share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    x = Dropout(0.2)(capsule)\n",
    "    x = PReLU()(Dense(128)(x))\n",
    "    \n",
    "    \n",
    "    x = Dense(19, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[inpword], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def deepmoji():\n",
    "    maxlen = 300\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 16\n",
    "    inpword = Input(shape=(300, ))\n",
    "    inparticle = Input(shape=(300, ))\n",
    "    embed_l2 = 0\n",
    "    embed_reg = L1L2(l2=embed_l2) if embed_l2 != 0 else None\n",
    "    x1 = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False, embeddings_regularizer=embed_reg)(inpword)\n",
    "    \n",
    "    x1 = SpatialDropout1D(0.4)(x1)\n",
    "    x1 = Reshape((config.maxlen, 600,  1))(x1)\n",
    "    \n",
    "    pooled_pre = []\n",
    "    for i in filter_sizes:\n",
    "    \n",
    "        conv_pre = Conv2D(num_filters, kernel_size=(i, 300), kernel_initializer='normal',\n",
    "                    activation='elu')(x1)\n",
    "    \n",
    "        maxpool_pre = MaxPool2D(pool_size=(maxlen - i + 1, 1))(conv_pre)\n",
    "        avepool_pre = AveragePooling2D(pool_size=(maxlen - i + 1, 1))(conv_pre)\n",
    "        globalmax_pre = GlobalMaxPooling2D()(conv_pre)\n",
    "        pooled_pre.append(globalmax_pre)\n",
    "        \n",
    "    z1 = Concatenate(axis=1)(pooled_pre)   \n",
    "    z1 = Dropout(0.2)(z1)\n",
    "    \n",
    "#     x1_post = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False, embeddings_regularizer=embed_reg)(inparticle)\n",
    "#     x1_post = SpatialDropout1D(0.4)(x1_post)\n",
    "#     x1_post = Reshape((config.maxlen, 600,  1))(x1_post)\n",
    "    \n",
    "#     pooled_post = []\n",
    "#     for i in filter_sizes:\n",
    "\n",
    "#         conv_post = Conv2D(num_filters, kernel_size=(i, 300), kernel_initializer='normal',\n",
    "#                     activation='elu')(x1_post)\n",
    "    \n",
    "#         maxpool_post = MaxPool2D(pool_size=(maxlen - i + 1, 1))(conv_post)\n",
    "#         avepool_post = AveragePooling2D(pool_size=(maxlen - i + 1, 1))(conv_post)\n",
    "#         globalmax_post = GlobalMaxPooling2D()(conv_post)\n",
    "#         pooled_post.append(globalmax_post)\n",
    "        \n",
    "#     z1_post = Concatenate(axis=1)(pooled_post)   \n",
    "#     z1_post = Dropout(0.2)(z1_post)\n",
    "#     z = concatenate([z1,z1_post])\n",
    "    x = Dense(19, activation=\"softmax\")(z1)\n",
    "    model = Model(inputs=[inpword, inparticle], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_y_train=np_utils.to_categorical(Y,num_classes=19)\n",
    "one_y_val=np_utils.to_categorical(Y,num_classes=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_dic = {0:0.001, 1:0.001, 2:0.0009, 3:0.0008, 4:0.0007, 5:0.0006, 6:0.0005, 7:0.0004, 8:0.0003, 9:0.0002, 10:0.0001,\n",
    "            11:0.00009, 12:0.00008, 13:0.00007, 14:0.00006, 15:0.00005}\n",
    "def lr_decay(epoch):\n",
    "    return decay_dic[epoch]\n",
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3, rnd=1):\n",
    "    now_nfold = 0\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233 * rnd)\n",
    "    train_pred, test_pred = np.zeros((102277, 19)), np.zeros((102277, 19))\n",
    "    LRDecay = LearningRateScheduler(lr_decay)\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        # x,y\n",
    "        now_nfold += 1\n",
    "        print(\"now is {} fold\".format(now_nfold))\n",
    "\n",
    "        curr_x1, curr_x2 = tr_word_pad[train_index], tr_word_post[train_index]\n",
    "        hold_out_x1, hold_out_x2 = tr_word_pad[test_index], tr_word_post[test_index]\n",
    "#         curr_x1 = tr_word_pad[train_index]\n",
    "#         hold_out_x1 = tr_word_pad[test_index]\n",
    "        curr_y, hold_out_y = one_y_train[train_index], one_y_train[test_index]\n",
    "\n",
    "#         kfold_X_features = features[train_index]\n",
    "#         kfold_X_valid_features = features[test_index]\n",
    "\n",
    "        config.batch_size=32\n",
    "        epochs = 15\n",
    "\n",
    "        model = deepmoji()\n",
    "\n",
    "        file_path = \"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, save_best_only=True, verbose=1, monitor='val_f1', mode='max')\n",
    "        early = EarlyStopping(monitor='val_f1', mode='max', patience=2, )\n",
    "        callbacks_list = [checkpoint, early, LRDecay]\n",
    "\n",
    "        model.fit([curr_x1, curr_x2], curr_y,\n",
    "                  batch_size=config.batch_size, epochs=epochs,\n",
    "                  validation_data=([hold_out_x1,hold_out_x2], hold_out_y),\n",
    "                  callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "        model.load_weights(file_path)\n",
    "\n",
    "        y_test = model.predict([te_word_pad, te_word_post])\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict([hold_out_x1, hold_out_x2])\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    try:\n",
    "        print('all eval', sqrt(mean_squared_error(Y, train_pred)))\n",
    "    finally:\n",
    "        return train_pred, test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "now is 1 fold\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      " 5152/92049 [>.............................] - ETA: 55:28 - loss: 10.5581 - f1: 0.0540"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d3a41d0dcc8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkf_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_cnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"[{}] finished nn model\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmodel_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-bd16658108b1>\u001b[0m in \u001b[0;36mkf_train\u001b[0;34m(fold_cnt, rnd)\u001b[0m\n\u001b[1;32m     36\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhold_out_x1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhold_out_x2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold_out_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                   callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "model_time = time.time()\n",
    "print(\"start\")\n",
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print (train_pred.shape,test_pred.shape)\n",
    "print (\"[{}] finished nn model\".format((time.time()-model_time)/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../feature/dpcnn1.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
