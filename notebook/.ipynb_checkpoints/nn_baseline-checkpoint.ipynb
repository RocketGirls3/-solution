{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from collections import Counter\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Input, Embedding, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import optimizers, losses, activations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from models_def import Attention\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "config = argparse.Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "der=False\n",
    "if der:\n",
    "    train_dir = '../input/new_data/train_set.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "else:\n",
    "#     train_dir = '../input/new_data/train_remove60.csv'\n",
    "#     test_dir = '../input/new_data/test_remove60.csv'\n",
    "    train_dir = '../input/new_data/train_pre_enhance.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "train = pd.read_csv(train_dir)\n",
    "test = pd.read_csv(test_dir)\n",
    "original_train = pd.read_csv('../input/new_data/train_set.csv')\n",
    "shuffle_data = train[original_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "ss\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    asd\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"s\")\n",
    "print(\"ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clear_text(df, num, col):\n",
    "    word_seg = df[col]\n",
    "    word_seg_list = word_seg.apply(lambda x: x.split(\" \"))\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for line in word_seg_list:\n",
    "        word_counts.update(line)\n",
    "\n",
    "    counter_list = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    label = list(map(lambda x: x[0], counter_list[:num]))\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(label):\n",
    "        for j in word_seg_list:\n",
    "            if i in j:\n",
    "                j.remove(i)\n",
    "    df[col] = word_seg_list\n",
    "    df[col] = df[col].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "train_offset = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_seg = all_data['word_seg']\n",
    "# word_seg_list = word_seg.apply(lambda x: x.split(\" \"))\n",
    "\n",
    "# word_counts = Counter()\n",
    "# for line in word_seg_list:\n",
    "#     word_counts.update(line)\n",
    "# counter_list = sorted(word_counts.items(), key=lambda x: x[1], reverse=False)\n",
    "# label = list(map(lambda x: x[0] if x[1] < 3 else 0, counter_list[:1000000]))\n",
    "# from tqdm import tqdm\n",
    "# for i in tqdm(label):\n",
    "#     if i == 0:\n",
    "#         continue\n",
    "#     for j in word_seg_list:\n",
    "#         if i in j:\n",
    "#             j.remove(i)\n",
    "# df['word_seg'] = word_seg_list\n",
    "# df['word_seg'] = df['word_seg'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if der:\n",
    "#     clear_text(all_data, 2, 'word_seg')\n",
    "\n",
    "\n",
    "# counter_list[:1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if der:\n",
    "#     clear_text(all_data, 50, 'article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "all_data['word_len'] = all_data['word_seg'].apply(len)\n",
    "all_data['word_unique'] = all_data['word_seg'].apply(lambda comment: len(set( w for w in comment.split())))\n",
    "all_data['word_unique_vs_len'] = all_data['word_unique'] /  all_data['word_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train = all_data.iloc[:train_offset,:]\n",
    "test = all_data.iloc[train_offset:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# features = train['word_unique_vs_len'].fillna(0)\n",
    "# test_features = test['word_unique_vs_len'].fillna(0)\n",
    "# features = features.reshape(-1, 1)\n",
    "# test_features = test_features.reshape(-1, 1)\n",
    "# ss = StandardScaler()\n",
    "# ss.fit(np.vstack((features, test_features)))\n",
    "# features = ss.transform(features)\n",
    "# test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('../input/train5.csv', index=False)\n",
    "# test.to_csv('../input/test5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_y_train = (train[\"class\"]-1).astype(int)\n",
    "# column = \"word_seg\"\n",
    "test_id = test[[\"id\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"word_seg\"\n",
    "config.len_desc = 800000\n",
    "tknzr_word = Tokenizer(num_words=config.len_desc)\n",
    "tknzr_word.fit_on_texts(all_data[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912182/912182 [00:01<00:00, 711669.09it/s]\n"
     ]
    }
   ],
   "source": [
    "count_thres = 5\n",
    "low_count_words = [w for w, c in tknzr_word.word_counts.items() if c < count_thres]\n",
    "# print(len(tknzr_word.texts_to_sequences(all_data[column].values)))\n",
    "for w in tqdm(low_count_words):\n",
    "    del tknzr_word.word_index[w]\n",
    "    del tknzr_word.word_docs[w]\n",
    "    del tknzr_word.word_counts[w]\n",
    "# print(len(tknzr_word.texts_to_sequences(all_data[column].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_word_seq = tknzr_word.texts_to_sequences(train[column].values)\n",
    "te_word_seq = tknzr_word.texts_to_sequences(test[column].values)\n",
    "config.maxlen = 2000\n",
    "tr_word_pad = pad_sequences(tr_word_seq, maxlen=config.maxlen)\n",
    "te_word_pad = pad_sequences(te_word_seq, maxlen=config.maxlen)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "max_features = 800000\n",
    "vec_len = 600\n",
    "EMBEDDING = '../feature/word2vec_file/avito600d.w2v'\n",
    "model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "word_index = tknzr_word.word_index\n",
    "nb_words_desc = min(max_features, len(word_index))\n",
    "embedding_matrix_desc = np.zeros((nb_words_desc+1, vec_len))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix_desc[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column = \"article\"\n",
    "# config.len_title = 100000\n",
    "# tknzr_article = Tokenizer(num_words=config.len_title)\n",
    "# tknzr_article.fit_on_texts(all_data[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_thres = 4\n",
    "# low_count_words = [w for w, c in tknzr_article.word_counts.items() if c < count_thres]\n",
    "# # print(len(tknzr_word.texts_to_sequences(all_data[column].values)))\n",
    "# for w in tqdm(low_count_words):\n",
    "#     del tknzr_article.word_index[w]\n",
    "#     del tknzr_article.word_docs[w]\n",
    "#     del tknzr_article.word_counts[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_article_seq = tknzr_article.texts_to_sequences(train[column].values)\n",
    "# te_article_seq = tknzr_article.texts_to_sequences(test[column].values)\n",
    "# config.titlemaxlen= 300\n",
    "# tr_article_pad = pad_sequences(tr_article_seq, maxlen=config.titlemaxlen)\n",
    "# te_article_pad = pad_sequences(te_article_seq, maxlen=config.titlemaxlen)\n",
    "# del all_data\n",
    "# gc.collect()\n",
    "# article_index = tknzr_article.word_index\n",
    "# nb_article = min(max_features, len(article_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max_features = 500000\n",
    "# vec_len = 600\n",
    "# EMBEDDING = \"../feature/doc2vec_file/doc2vec_600d.model\"\n",
    "# model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "# char_embedding_matrix = np.zeros((nb_article+1, vec_len))\n",
    "# for word, i in article_index.items():\n",
    "#     if i >= max_features: continue\n",
    "#     try:\n",
    "#         embedding_vector = model[word]\n",
    "#     except KeyError:\n",
    "#         embedding_vector = None\n",
    "#     if embedding_vector is not None: char_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "# embeddings_index = {}\n",
    "# with open('../feature/word2vec_file/char_embed.own','r') as f:\n",
    "#     for i in f:\n",
    "#         values = i.split(' ')\n",
    "#         if len(values) == 2: continue\n",
    "#         word = str(values[0])\n",
    "#         embedding = np.asarray(values[1:],dtype='float')\n",
    "#         embeddings_index[word] = embedding\n",
    "# print('char embedding',len(embeddings_index))\n",
    "\n",
    "# EMBEDDING_DIM = 300\n",
    "# char_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     if i > max_features: continue\n",
    "#     embedding_vector = embeddings_index.get(str(word).upper())\n",
    "#     if embedding_vector is not None:\n",
    "#         char_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "# embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "# max_features = 30000\n",
    "# embed_size=300\n",
    "# word_index = tknzr_word.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "# embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_index = tknzr_article.word_index\n",
    "# nb_article_desc = min(max_features, len(article_index))\n",
    "\n",
    "\n",
    "# word_index = tknzr_word.word_index\n",
    "# nb_words  =min(max_features, len(word_idx))\n",
    "# nb_words = len(word_index)\n",
    "\n",
    "# article_idx = tknzr_article.word_index\n",
    "# # nb_article = min(max_features, len(article_idx))\n",
    "# nb_article = len(article_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_idx = df_y_train.sample(frac=0.1, random_state=1991).index\n",
    "# train_idx = df_y_train[np.invert(df_y_train.index.isin(valid_idx))].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([tr_word_pad, tr_article_pad])\n",
    "# # X = tr_word_pad\n",
    "# X_test = np.array([te_word_pad, te_article_pad])\n",
    "# # X_test = te_word_pad\n",
    "# X_test = [np.array(i)for i in X_test]\n",
    "Y = df_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = [x[train_idx] for x in X]\n",
    "# X_valid = [x[valid_idx] for x in X]\n",
    "# X_test = [x for x in X_test]\n",
    "\n",
    "# Y_train = Y[train_idx]\n",
    "# Y_valid = Y[valid_idx]\n",
    "# X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.engine import InputSpec, Layer\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen=600\n",
    "config.maxlen=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../feature/midel3.pkl','wb') as fout:\n",
    "#     pickle.dump([X_train,X_valid,X_test,Y_train,Y_valid, nb_words_desc, embedding_matrix_desc, ],fout)\n",
    "# X_train,X_valid,X_test,Y_train,Y_valid, nb_words_desc, embedding_matrix_desc= pickle.load(open('../feature/midel2.pkl','rb'))\n",
    "\n",
    "# with open('../feature/midel3.pkl','wb') as fout:\n",
    "#     pickle.dump([ config.maxlen, train, tr_word_pad,  te_word_pad,embedding_matrix_desc],fout)\n",
    "# config.maxlen, train, tr_word_pad,  te_word_pad,embedding_matrix_desc= pickle.load(open('../feature/midel3.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('../feature/midel.pkl','wb') as fout:\n",
    "#     pickle.dump([X_train,X_valid,X_test,Y_train,Y_valid, nb_words, nb_article],fout)\n",
    "# X_train,X_valid,X_test,Y_train,Y_valid, nb_words, nb_article= pickle.load(open('../feature/midel.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras.layers import Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        \n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "        \n",
    "        # return flattened output\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dense\n",
    "# from keras.layers import GlobalMaxPool1D, GlobalMaxPool2D, SpatialDropout1D, Dropout, BatchNormalization, Lambda\n",
    "# from keras.layers import concatenate, Flatten, add, dot, PReLU, merge, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Add, Multiply\n",
    "# from keras.layers import LSTM, Conv1D, GlobalMaxPool2D, Convolution2D, Conv2D, CuDNNGRU, CuDNNLSTM, GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras import optimizers, losses, activations\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers import *\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "def get_rnn_and_cnn_model():\n",
    "#     features_input = Input(shape=(features.shape[1],))\n",
    "    \n",
    "    inparticle = Input(shape=(config.titlemaxlen, ))\n",
    "    emb_article = Embedding(char_embedding_matrix.shape[0], 600,  weights = [char_embedding_matrix],trainable=False)(inparticle)\n",
    "    \n",
    "    inpword = Input(shape=(config.maxlen, ))\n",
    "    emb_word = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)\n",
    "\n",
    "    \n",
    "    lDropout_titl = SpatialDropout1D(0.5)(emb_word)\n",
    "    title_layer = Bidirectional(CuDNNLSTM(128, return_sequences=True))(lDropout_titl)\n",
    "\n",
    "    title_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))(title_layer)\n",
    "\n",
    "    max_pool_til = GlobalMaxPooling1D()(title_layer)\n",
    "    att = AttentionWeightedAverage()(title_layer)\n",
    "    \n",
    "    lDropout_article = SpatialDropout1D(0.5)(emb_article)\n",
    "    convs=[]\n",
    "    for i in range(1,8):\n",
    "        conv = Conv1D(64,kernel_size=i,padding='valid')(lDropout_article)\n",
    "        conv = PReLU()(conv)\n",
    "        conv = Dropout(0.2)(conv)\n",
    "        conv = GlobalMaxPooling1D()(conv)\n",
    "        convs.append(conv)\n",
    "    char_merged=concatenate(convs)\n",
    "    merged = concatenate([max_pool_til, att,char_merged])\n",
    "    x = Dropout(0.2)(merged)\n",
    "    \n",
    "    x = PReLU()(Dense(256)(x))\n",
    "    x = Dense(20, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[inpword, inparticle], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_rnn_model():\n",
    "#     features_input = Input(shape=(features.shape[1],))\n",
    "\n",
    "    \n",
    "    inpword = Input(shape=(1000, ))\n",
    "    emb_word = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)\n",
    "\n",
    "    lDropout_titl = SpatialDropout1D(0.5)(emb_word)\n",
    "    title_layer = Bidirectional(CuDNNLSTM(128, return_sequences=True))(lDropout_titl)\n",
    "\n",
    "    title_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))(title_layer)\n",
    "\n",
    "    max_pool_til = GlobalMaxPooling1D()(title_layer)\n",
    "    att = AttentionWeightedAverage()(title_layer)\n",
    "# AttentionWeightedAverage()\n",
    "\n",
    "    all_views = concatenate([max_pool_til, att], axis=1)\n",
    "    x = Dropout(0.5)(all_views)\n",
    "\n",
    "    x = PReLU()(Dense(128)(x))\n",
    "\n",
    "    x = Dense(19, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[inpword], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_Capsule_model():\n",
    "    features_input = Input(shape=(features.shape[1],))\n",
    "\n",
    "    inpword = Input(shape=(config.maxlen, ))\n",
    "    emb_word = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)\n",
    "    embed_layer = SpatialDropout1D(0.4)(emb_word)\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(embed_layer)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    capsule = Capsule(num_capsule=10, dim_capsule=32, routings=5,share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    x = Dropout(0.2)(capsule)\n",
    "    x = PReLU()(Dense(128)(x))\n",
    "    \n",
    "    \n",
    "    x = Dense(20, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[inpword, features_input], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_cnn():\n",
    "    def conv_block(x, n, kernel_size):\n",
    "        x = Conv1D(n, kernel_size, activation='relu') (x)\n",
    "        x = Conv1D(n_filters, kernel_size, activation='relu') (x)\n",
    "        x_att = AttentionWeightedAverage()(x)\n",
    "#         x_avg = GlobalAvgPool1D()(x)\n",
    "        x_max = GlobalMaxPool1D()(x)\n",
    "        return concatenate([x_att, x_max])    \n",
    "    n_filters = 64\n",
    "    inpword = Input(shape=(config.maxlen, ))\n",
    "    emb_word = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)     \n",
    "    x = SpatialDropout1D(0.3)(emb_word)\n",
    "    x1 = conv_block(x, 4*n_filters, 2)\n",
    "    x2 = conv_block(x, 3*n_filters, 3)\n",
    "    x3 = conv_block(x, 2*n_filters, 4)\n",
    "    x = concatenate([x1, x2, x3])\n",
    "    x = Dense(20, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inpword, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='nadam',\n",
    "                    metrics=[f1])\n",
    "    return model\n",
    "print('def model done')\n",
    "\n",
    "def get_mlp():\n",
    "    inpword = Input(shape=(1000, ))\n",
    "    emb_word = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)\n",
    "    lDropout_titl = SpatialDropout1D(0.1)(emb_word)\n",
    "    x = Dense(100, activation='sigmoid')(lDropout_titl)\n",
    "    x3 = Dense(960)(emb_word)\n",
    "    x3 = PReLU()(x3)\n",
    "    x1 = concatenate([x, x3])\n",
    "    x1 = Dropout(0.1)(x1)\n",
    "    x = Dense(512, activation='sigmoid')(x1)\n",
    "    x2 = Dense(11, activation='linear')(x1)\n",
    "    x3 = Dense(11)(x1)\n",
    "    x3 = PReLU()(x3)\n",
    "    x1 = concatenate([x, x2, x3])\n",
    "    x = Dropout(0.1)(x1)\n",
    "#     avg_pool = GlobalAveragePooling1D()(x)\n",
    "    att = AttentionWeightedAverage()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    merged = concatenate([att, max_pool])\n",
    "    x = Dropout(0.1)(merged)\n",
    "    x = Dense(19, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[inpword], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_y_train=np_utils.to_categorical(Y,num_classes=19)\n",
    "one_y_val=np_utils.to_categorical(Y,num_classes=19)\n",
    "# model = get_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_dic = {0:0.001, 1:0.001, 2:0.0009, 3:0.0008, 4:0.0007, 5:0.0006, 6:0.0005, 7:0.0004, 8:0.0003, 9:0.0002, 10:0.0001,\n",
    "            11:0.00009, 12:0.00008, 13:0.00007, 14:0.00006, 15:0.00005}\n",
    "def lr_decay(epoch):\n",
    "    return decay_dic[epoch]\n",
    "from sklearn.model_selection import KFold\n",
    "# pretrain=False\n",
    "# if pretrain == True and len(X_test)<38:\n",
    "#     X_test.append(test_x)\n",
    "# def kf_train(fold_cnt=3,rnd=1):\n",
    "#     now_nfold=0\n",
    "#     kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "#     train_pred, test_pred = np.zeros((102277,19)),np.zeros((102277,19))\n",
    "#     LRDecay = LearningRateScheduler(lr_decay)\n",
    "# #     train_pred, test_pred = np.zeros((train_p_s,1)),np.zeros((test_p_s,1))\n",
    "#     for train_index, test_index in kf.split(original_train):\n",
    "#         # x,y\n",
    "        \n",
    "#         now_nfold+=1\n",
    "#         print (\"now is {} fold\".format(now_nfold))\n",
    "# #         curr_x1, curr_x2 = tr_word_pad[train_index], tr_article_pad[train_index]\n",
    "#         curr_x1 = tr_word_pad[train_index]\n",
    "# #         hold_out_x1,hold_out_x2 =tr_word_pad[test_index],tr_article_pad[test_index]\n",
    "#         hold_out_x1=tr_word_pad[test_index]\n",
    "#         shuffle_number = 1\n",
    "#         shuffle_word_pad = tr_word_pad[original_train.shape[0]:]\n",
    "#         shuffle_word_pad_y = one_y_train[original_train.shape[0]:]\n",
    "#         for shuffle_train_index, shuffle_test_index in kf.split(shuffle_data):\n",
    "#             shuffle_train = shuffle_word_pad[shuffle_train_index]\n",
    "#             shuffle_train_y = shuffle_word_pad_y[shuffle_train_index]\n",
    "#             if shuffle_number == now_nfold:\n",
    "#                 break\n",
    "#             shuffle_number+=1\n",
    "#         curr_x1 = np.vstack((curr_x1, shuffle_train))\n",
    "#         curr_y, hold_out_y = one_y_train[train_index], one_y_train[test_index]\n",
    "#         curr_y = np.vstack((curr_y, shuffle_train_y))\n",
    "# #         curr_x,curr_y = X[train_index],one_y_train[train_index]\n",
    "# #         hold_out_x,hold_out_y =X[test_index],one_y_val[test_index]\n",
    "# #         curr_x,curr_y = [np.array(x)[train_index] for x in X],one_y_train[train_index]\n",
    "# #         curr_other_x = train_x[train_index]\n",
    "# #         hold_out_x,hold_out_y = [np.array(x)[test_index] for x in X],one_y_train[test_index]\n",
    "# #         hold_out_other_x = train_x[test_index]\n",
    "# #         kfold_X_features = features[train_index]\n",
    "# #         kfold_X_valid_features = features[test_index]\n",
    "#         # curr_x ,curr_y= [x[train_idx] for x in X]\n",
    "#         # hold_out_x = [x[valid_idx] for x in X]\n",
    "#         # model\n",
    "#         config.batch_size = 64\n",
    "#         epochs = 15\n",
    "\n",
    "#         model = get_rnn_and_cnn_model()\n",
    "\n",
    "#         file_path=\"weights_base.best.h5\"\n",
    "#         checkpoint = ModelCheckpoint(file_path, save_best_only=True,verbose=1, monitor='val_f1',  mode='max')\n",
    "#         early = EarlyStopping( monitor='val_f1',  mode='max', patience=2,)\n",
    "#         callbacks_list = [checkpoint,early,LRDecay]\n",
    "#         # train and pred\n",
    "# #         if pretrain == False:\n",
    "#         model.fit(curr_x1, curr_y, \n",
    "#                   batch_size=config.batch_size, epochs=epochs, \n",
    "#                     validation_data=(hold_out_x1, hold_out_y), \n",
    "#                   callbacks=callbacks_list)\n",
    "# #         else:\n",
    "# #             curr_x.append(curr_other_x)\n",
    "# #             hold_out_x.append(hold_out_other_x)\n",
    "            \n",
    "# #             model.fit(X_test, test_y,\n",
    "# #             batch_size=config.batch_size, epochs=1,\n",
    "# #             validation_data=(hold_out_x, hold_out_y),\n",
    "# #             callbacks=callbacks_list)\n",
    "            \n",
    "            \n",
    "\n",
    "# #             model.fit(curr_x,  curr_y, \n",
    "# #                       batch_size=config.batch_size, epochs=epochs, \n",
    "# #                       validation_data=\n",
    "# #                       (hold_out_x, hold_out_y), \n",
    "# #                       callbacks=callbacks_list)\n",
    "        \n",
    "#         model.load_weights(file_path)\n",
    "       \n",
    "#         y_test = model.predict(te_word_pad)\n",
    "#         test_pred += y_test\n",
    "#         hold_out_pred = model.predict(hold_out_x1)\n",
    "#         train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "#         # clear\n",
    "#         del model\n",
    "#         gc.collect()\n",
    "#         K.clear_session()\n",
    "#     test_pred = test_pred / fold_cnt\n",
    "#     print('-------------------------------')\n",
    "#     try:\n",
    "#         print('all eval',sqrt(mean_squared_error(Y,train_pred)))\n",
    "#     finally:\n",
    "#         return train_pred, test_pred\n",
    "\n",
    "\n",
    "# print('def done')\n",
    "def kf_train(fold_cnt=3, rnd=1):\n",
    "    now_nfold = 0\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233 * rnd)\n",
    "    train_pred, test_pred = np.zeros((102277, 19)), np.zeros((102277, 19))\n",
    "    LRDecay = LearningRateScheduler(lr_decay)\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        # x,y\n",
    "        now_nfold += 1\n",
    "        print(\"now is {} fold\".format(now_nfold))\n",
    "\n",
    "        curr_x1 = tr_word_pad[train_index]\n",
    "\n",
    "        hold_out_x1 = tr_word_pad[test_index]\n",
    "        curr_y, hold_out_y = one_y_train[train_index], one_y_train[test_index]\n",
    "\n",
    "#         kfold_X_features = features[train_index]\n",
    "#         kfold_X_valid_features = features[test_index]\n",
    "\n",
    "        config.batch_size=8\n",
    "        epochs = 15\n",
    "\n",
    "        model = get_rnn_model()\n",
    "\n",
    "        file_path = \"weights_base_rnn_s.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, save_best_only=True, verbose=1, monitor='val_f1', mode='max')\n",
    "        early = EarlyStopping(monitor='val_f1', mode='max', patience=2, )\n",
    "        callbacks_list = [checkpoint, early, LRDecay]\n",
    "\n",
    "        model.fit(curr_x1, curr_y,\n",
    "                  batch_size=config.batch_size, epochs=epochs,\n",
    "                  validation_data=(hold_out_x1, hold_out_y),\n",
    "                  callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "        model.load_weights(file_path)\n",
    "\n",
    "        y_test = model.predict(te_word_pad)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x1)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    try:\n",
    "        print('all eval', sqrt(mean_squared_error(Y, train_pred)))\n",
    "    finally:\n",
    "        return train_pred, test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# cvlist2 = list(StratifiedKFold(n_splits=10, random_state=786).split(tr_word_pad, Y))\n",
    "# def shuffle_train_predict(cvlist, X, y, X_test, lr_decay):\n",
    "#     y_trues = []\n",
    "#     y_preds = []\n",
    "#     y_test_preds = []\n",
    "#     scores = []\n",
    "#     LRDecay = LearningRateScheduler(lr_decay)\n",
    "\n",
    "#     for tr_index, val_index in cvlist:\n",
    "#         X_tr, y_tr = X[tr_index, :], y[tr_index, :]\n",
    "#         X_val, y_val = X[val_index, :], y[val_index, :]\n",
    "#         config.batch_size = 64\n",
    "#         epochs = 15\n",
    "\n",
    "#         model = get_rnn_model()\n",
    "\n",
    "#         file_path=\"weights_base.best.h5\"\n",
    "#         checkpoint = ModelCheckpoint(file_path, save_best_only=True,verbose=1, monitor='val_f1',  mode='max')\n",
    "#         early = EarlyStopping( monitor='val_f1',  mode='max', patience=2,)\n",
    "#         callbacks_list = [checkpoint,early,LRDecay]\n",
    "#         # train and pred\n",
    "# #         model.set_params(**{'callbacks':[]})\n",
    "#         model.fit(X_tr, y_tr,\n",
    "#                                    batch_size=config.batch_size, epochs=epochs, \n",
    "#                     validation_data=(X_val, y_val), \n",
    "#                   callbacks=callbacks_list)\n",
    "\n",
    "#         y_pred = model.predict(X_val)\n",
    "#         y_test_pred = model.predict(X_test)\n",
    "#         score = f1_score(y_val, y_pred,  average='macro')\n",
    "#         scores.append(score)\n",
    "#         print(\"f1 for this fold is \", score)\n",
    "#         y_trues.append(y_val)\n",
    "#         y_preds.append(y_pred)\n",
    "#         y_test_preds.append(y_test_pred)\n",
    "#         K.clear_session()\n",
    "#         gc.collect()\n",
    "#         #break\n",
    "#     y_trues = np.concatenate(y_trues)\n",
    "#     y_preds = np.concatenate(y_preds)\n",
    "#     y_test_preds = np.mean(y_test_preds, axis=0)\n",
    "#     print(\"Shape of test _preds is \", y_test_preds.shape)\n",
    "#     print(\"Means of val and test preds are {} and {}\".format(np.mean(y_preds, axis=1), np.mean(y_test_preds, axis=1)))\n",
    "#     score = roc_auc_score(y_trues, y_preds)\n",
    "#     print(\"Overall score on 10 fold CV is {}\".format(score))\n",
    "    \n",
    "#     return y_preds, y_trues, y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "now is 1 fold\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (1000,) but got array with shape (2000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-866cc336cf1e>\u001b[0m in \u001b[0;36mkf_train\u001b[0;34m(fold_cnt, rnd)\u001b[0m\n\u001b[1;32m    128\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhold_out_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold_out_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                   callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (1000,) but got array with shape (2000,)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import pickle\n",
    "model_time = time.time()\n",
    "print(\"start\")\n",
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print (train_pred.shape,test_pred.shape)\n",
    "print (\"[{}] finished nn model\".format((time.time()-model_time)/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_preds, y_trues, y_test_preds = shuffle_train_predict(cvlist2, tr_word_pad, one_y_train, te_word_pad, lr_decay) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = get_model()\n",
    "# batch_size = 64\n",
    "# epochs = 15\n",
    "# one_y_train=np_utils.to_categorical(Y_train,num_classes=20)\n",
    "# one_y_val=np_utils.to_categorical(Y_valid,num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = get_model()\n",
    "# batch_size = 64\n",
    "# epochs = 15\n",
    "# one_y_train=np_utils.to_categorical(Y_train,num_classes=20)\n",
    "# one_y_val=np_utils.to_categorical(Y_valid,num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file_path=\"weights_base01.best.h5\"\n",
    "# checkpoint = ModelCheckpoint(file_path, save_best_only=True,verbose=1, monitor='val_f1',  mode='max')\n",
    "# early = EarlyStopping( monitor='val_f1',  mode='max', patience=2,)\n",
    "# callbacks_list = [checkpoint,early]\n",
    "\n",
    "# model.fit(X_train, one_y_train, \n",
    "#           batch_size=batch_size, epochs=epochs, \n",
    "#           validation_data=(X_valid,one_y_val),\n",
    "#              callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f8a9b53b7a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# val_test = model.predict(X_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../feature/rnn_doc2vec_base.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# val_test = model.predict(X_valid)\n",
    "with open('../feature/rnn_doc2vec_base.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result = np.argmax(train_pred,axis=1)\n",
    "print ('score is {}'.format(f1_score(Y, val_result, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob=pd.DataFrame(test_pred)\n",
    "test_prob.columns=[\"class_prob_%s\"%i for i in range(1,test_pred.shape[1]+1)]\n",
    "test_prob[\"id\"]=list(test_id[\"id\"])\n",
    "test_prob.to_csv('../pro/prob_rnn_doc2vec_base.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=np.argmax(test_pred,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"class\"]\n",
    "test_pred[\"class\"]=(test_pred[\"class\"]+1).astype(int)\n",
    "print(test_pred.shape)\n",
    "print(test_id.shape)\n",
    "test_pred[\"id\"]=list(test_id[\"id\"])\n",
    "test_pred[[\"id\",\"class\"]].to_csv('../output/sub_rnn_doc2vec_base.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
