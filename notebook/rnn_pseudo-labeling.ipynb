{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from collections import Counter\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Input, Embedding, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import optimizers, losses, activations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from models_def import Attention\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "config = argparse.Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "der=True\n",
    "if der:\n",
    "    train_dir = '../input/new_data/train_set.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "else:\n",
    "#     train_dir = '../input/new_data/train_remove60.csv'\n",
    "#     test_dir = '../input/new_data/test_remove60.csv'\n",
    "    train_dir = '../input/new_data/shuffle_train.csv'\n",
    "    test_dir = '../input/new_data/test_set.csv'\n",
    "\n",
    "train = pd.read_csv(train_dir)\n",
    "test = pd.read_csv(test_dir)\n",
    "# original_train = pd.read_csv('../input/new_data/train_set.csv')\n",
    "# shuffle_data = train[original_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "df_y_train = (train[\"class\"]-1).astype(int)\n",
    "test_id = test[[\"id\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_label_data = pd.read_csv('../10fold_pro/prob_rnn_baseline4.csv')\n",
    "name = [\"class_prob_%s\"%i for i in range(1,20)]\n",
    "pseudo_y=np.argmax(pseudo_label_data[name].values,axis=1)\n",
    "# all_y = np.append(df_y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "train_offset = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column = \"word_seg\"\n",
    "config.len_desc = 800000\n",
    "tknzr_word = Tokenizer(num_words=config.len_desc)\n",
    "tknzr_word.fit_on_texts(all_data[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912182/912182 [00:01<00:00, 731547.37it/s]\n"
     ]
    }
   ],
   "source": [
    "count_thres = 5\n",
    "low_count_words = [w for w, c in tknzr_word.word_counts.items() if c < count_thres]\n",
    "# print(len(tknzr_word.texts_to_sequences(all_data[column].values)))\n",
    "for w in tqdm(low_count_words):\n",
    "    del tknzr_word.word_index[w]\n",
    "    del tknzr_word.word_docs[w]\n",
    "    del tknzr_word.word_counts[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_word_seq = tknzr_word.texts_to_sequences(train[column].values)\n",
    "te_word_seq = tknzr_word.texts_to_sequences(test[column].values)\n",
    "config.maxlen = 1000\n",
    "tr_word_pad = pad_sequences(tr_word_seq, maxlen=config.maxlen)\n",
    "te_word_pad = pad_sequences(te_word_seq, maxlen=config.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "max_features = 800000\n",
    "vec_len = 600\n",
    "EMBEDDING = '../feature/word2vec_file/avito600d.w2v'\n",
    "model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "word_index = tknzr_word.word_index\n",
    "nb_words_desc = min(max_features, len(word_index))\n",
    "embedding_matrix_desc = np.zeros((nb_words_desc+1, vec_len))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix_desc[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_y_train\n",
    "pseudo_Y = pseudo_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import InputSpec, Layer\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dense\n",
    "# from keras.layers import GlobalMaxPool1D, GlobalMaxPool2D, SpatialDropout1D, Dropout, BatchNormalization, Lambda\n",
    "# from keras.layers import concatenate, Flatten, add, dot, PReLU, merge, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Add, Multiply\n",
    "# from keras.layers import LSTM, Conv1D, GlobalMaxPool2D, Convolution2D, Conv2D, CuDNNGRU, CuDNNLSTM, GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras import optimizers, losses, activations\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers import *\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_model():\n",
    "#     features_input = Input(shape=(features.shape[1],))\n",
    "\n",
    "    \n",
    "    inpword = Input(shape=(config.maxlen, ))\n",
    "    emb_word = Embedding(embedding_matrix_desc.shape[0], 600,  weights = [embedding_matrix_desc],trainable=False)(inpword)\n",
    "\n",
    "    lDropout_titl = SpatialDropout1D(0.5)(emb_word)\n",
    "    title_layer = Bidirectional(CuDNNLSTM(128, return_sequences=True))(lDropout_titl)\n",
    "\n",
    "    title_layer = Bidirectional(CuDNNGRU(128, return_sequences=True))(title_layer)\n",
    "\n",
    "    max_pool_til = GlobalMaxPooling1D()(title_layer)\n",
    "    att = AttentionWeightedAverage()(title_layer)\n",
    "# AttentionWeightedAverage()\n",
    "\n",
    "    all_views = concatenate([max_pool_til, att], axis=1)\n",
    "    x = Dropout(0.5)(all_views)\n",
    "\n",
    "    x = PReLU()(Dense(128)(x))\n",
    "\n",
    "    x = Dense(19, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[inpword], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                    metrics=[f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_y_train=np_utils.to_categorical(df_y_train,num_classes=19)\n",
    "pseudo_y_train=np_utils.to_categorical(pseudo_Y,num_classes=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "decay_dic = {0:0.001, 1:0.001, 2:0.0009, 3:0.0008, 4:0.0007, 5:0.0006, 6:0.0005, 7:0.0004, 8:0.0003, 9:0.0002, 10:0.0001,\n",
    "            11:0.00009, 12:0.00008, 13:0.00007, 14:0.00006, 15:0.00005}\n",
    "def lr_decay(epoch):\n",
    "    return decay_dic[epoch]\n",
    "from sklearn.model_selection import KFold\n",
    "# pretrain=False\n",
    "# if pretrain == True and len(X_test)<38:\n",
    "#     X_test.append(test_x)\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    now_nfold=0\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((102277,19)),np.zeros((102277,19))\n",
    "    LRDecay = LearningRateScheduler(lr_decay)\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        now_nfold+=1\n",
    "        print (\"now is {} fold\".format(now_nfold))\n",
    "        curr_x1 = tr_word_pad[train_index]\n",
    "        hold_out_x1=tr_word_pad[test_index]\n",
    "        pseudo_number = 1\n",
    "#         pseudo_word_pad = tr_word_pad[original_train.shape[0]:]\n",
    "#         pseudo_word_pad_y = one_y_train[original_train.shape[0]:]\n",
    "        for pseudo_train_index, pseudo_test_index in kf.split(te_word_pad):\n",
    "            pseudo_train = te_word_pad[pseudo_train_index]\n",
    "            pseudo_train_y = pseudo_y_train[pseudo_train_index]\n",
    "            if pseudo_number == now_nfold:\n",
    "                break\n",
    "            pseudo_number+=1\n",
    "        curr_x1 = np.vstack((curr_x1, pseudo_train))\n",
    "        curr_y, hold_out_y = one_y_train[train_index], one_y_train[test_index]\n",
    "        curr_y = np.vstack((curr_y, pseudo_train_y))\n",
    "\n",
    "        config.batch_size = 64\n",
    "        epochs = 15\n",
    "\n",
    "        model = get_rnn_model()\n",
    "\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, save_best_only=True,verbose=1, monitor='val_f1',  mode='max')\n",
    "        early = EarlyStopping( monitor='val_f1',  mode='max', patience=2,)\n",
    "        callbacks_list = [checkpoint,early,LRDecay]\n",
    "\n",
    "        model.fit(curr_x1, curr_y, \n",
    "                  batch_size=config.batch_size, epochs=epochs, \n",
    "                    validation_data=(hold_out_x1, hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "       \n",
    "        y_test = model.predict(te_word_pad)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x1)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    try:\n",
    "        print('all eval',sqrt(mean_squared_error(Y,train_pred)))\n",
    "    finally:\n",
    "        return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now is 1 fold\n",
      "Train on 184098 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "184098/184098 [==============================] - 1409s 8ms/step - loss: 0.8742 - f1: 0.7290 - val_loss: 0.8687 - val_f1: 0.7700\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.77001, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184098/184098 [==============================] - 1425s 8ms/step - loss: 0.6443 - f1: 0.8091 - val_loss: 0.8235 - val_f1: 0.7816\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.77001 to 0.78165, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184098/184098 [==============================] - 1437s 8ms/step - loss: 0.5891 - f1: 0.8260 - val_loss: 0.8031 - val_f1: 0.7909\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.78165 to 0.79090, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184098/184098 [==============================] - 1441s 8ms/step - loss: 0.5537 - f1: 0.8376 - val_loss: 0.7914 - val_f1: 0.7958\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.79090 to 0.79577, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184098/184098 [==============================] - 1444s 8ms/step - loss: 0.5257 - f1: 0.8466 - val_loss: 0.7829 - val_f1: 0.7979\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.79577 to 0.79792, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184098/184098 [==============================] - 1444s 8ms/step - loss: 0.5033 - f1: 0.8529 - val_loss: 0.7843 - val_f1: 0.8018\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.79792 to 0.80181, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184098/184098 [==============================] - 1443s 8ms/step - loss: 0.4848 - f1: 0.8590 - val_loss: 0.7828 - val_f1: 0.8007\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.80181\n",
      "Epoch 8/15\n",
      "184098/184098 [==============================] - 1444s 8ms/step - loss: 0.4676 - f1: 0.8641 - val_loss: 0.7731 - val_f1: 0.8017\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.80181\n",
      "now is 2 fold\n",
      "Train on 184098 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "184098/184098 [==============================] - 1436s 8ms/step - loss: 0.8720 - f1: 0.7303 - val_loss: 0.8441 - val_f1: 0.7712\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.77119, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184098/184098 [==============================] - 1442s 8ms/step - loss: 0.6465 - f1: 0.8077 - val_loss: 0.8099 - val_f1: 0.7813\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.77119 to 0.78133, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184098/184098 [==============================] - 1441s 8ms/step - loss: 0.5877 - f1: 0.8269 - val_loss: 0.7516 - val_f1: 0.7923\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.78133 to 0.79226, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184098/184098 [==============================] - 1440s 8ms/step - loss: 0.5545 - f1: 0.8371 - val_loss: 0.7415 - val_f1: 0.7957\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.79226 to 0.79573, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184098/184098 [==============================] - 1439s 8ms/step - loss: 0.5271 - f1: 0.8456 - val_loss: 0.7462 - val_f1: 0.7985\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.79573 to 0.79846, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184098/184098 [==============================] - 1440s 8ms/step - loss: 0.5057 - f1: 0.8535 - val_loss: 0.7586 - val_f1: 0.7971\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.79846\n",
      "Epoch 7/15\n",
      "184098/184098 [==============================] - 1441s 8ms/step - loss: 0.4849 - f1: 0.8594 - val_loss: 0.7382 - val_f1: 0.8028\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.79846 to 0.80276, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "184098/184098 [==============================] - 1440s 8ms/step - loss: 0.4690 - f1: 0.8644 - val_loss: 0.7420 - val_f1: 0.8040\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.80276 to 0.80399, saving model to weights_base.best.h5\n",
      "Epoch 9/15\n",
      "184098/184098 [==============================] - 1439s 8ms/step - loss: 0.4525 - f1: 0.8689 - val_loss: 0.7389 - val_f1: 0.8039\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.80399\n",
      "Epoch 10/15\n",
      "184098/184098 [==============================] - 1439s 8ms/step - loss: 0.4388 - f1: 0.8733 - val_loss: 0.7334 - val_f1: 0.8050\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.80399 to 0.80498, saving model to weights_base.best.h5\n",
      "Epoch 11/15\n",
      "184098/184098 [==============================] - 1438s 8ms/step - loss: 0.4274 - f1: 0.8770 - val_loss: 0.7381 - val_f1: 0.8057\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.80498 to 0.80568, saving model to weights_base.best.h5\n",
      "Epoch 12/15\n",
      "184098/184098 [==============================] - 1437s 8ms/step - loss: 0.4207 - f1: 0.8791 - val_loss: 0.7402 - val_f1: 0.8065\n",
      "\n",
      "Epoch 00012: val_f1 improved from 0.80568 to 0.80652, saving model to weights_base.best.h5\n",
      "Epoch 13/15\n",
      "184098/184098 [==============================] - 1437s 8ms/step - loss: 0.4170 - f1: 0.8798 - val_loss: 0.7389 - val_f1: 0.8055\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.80652\n",
      "Epoch 14/15\n",
      "184098/184098 [==============================] - 1437s 8ms/step - loss: 0.4148 - f1: 0.8810 - val_loss: 0.7367 - val_f1: 0.8071\n",
      "\n",
      "Epoch 00014: val_f1 improved from 0.80652 to 0.80707, saving model to weights_base.best.h5\n",
      "Epoch 15/15\n",
      "184098/184098 [==============================] - 1437s 8ms/step - loss: 0.4110 - f1: 0.8817 - val_loss: 0.7386 - val_f1: 0.8061\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.80707\n",
      "now is 3 fold\n",
      "Train on 184098 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "184098/184098 [==============================] - 1434s 8ms/step - loss: 0.8801 - f1: 0.7271 - val_loss: 0.8495 - val_f1: 0.7734\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.77340, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184098/184098 [==============================] - 1437s 8ms/step - loss: 0.6525 - f1: 0.8075 - val_loss: 0.7910 - val_f1: 0.7873\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.77340 to 0.78731, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184098/184098 [==============================] - 1431s 8ms/step - loss: 0.5929 - f1: 0.8256 - val_loss: 0.7750 - val_f1: 0.7928\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.78731 to 0.79282, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184098/184098 [==============================] - 1430s 8ms/step - loss: 0.5570 - f1: 0.8375 - val_loss: 0.7662 - val_f1: 0.7944\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.79282 to 0.79439, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184098/184098 [==============================] - 1432s 8ms/step - loss: 0.5298 - f1: 0.8450 - val_loss: 0.7682 - val_f1: 0.7973\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.79439 to 0.79728, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184098/184098 [==============================] - 1436s 8ms/step - loss: 0.5072 - f1: 0.8530 - val_loss: 0.7542 - val_f1: 0.8005\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.79728 to 0.80050, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184098/184098 [==============================] - 1443s 8ms/step - loss: 0.4881 - f1: 0.8583 - val_loss: 0.7531 - val_f1: 0.8008\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.80050 to 0.80077, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "184098/184098 [==============================] - 1448s 8ms/step - loss: 0.4696 - f1: 0.8640 - val_loss: 0.7586 - val_f1: 0.8042\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.80077 to 0.80425, saving model to weights_base.best.h5\n",
      "Epoch 9/15\n",
      "184098/184098 [==============================] - 1448s 8ms/step - loss: 0.4568 - f1: 0.8674 - val_loss: 0.7476 - val_f1: 0.8048\n",
      "\n",
      "Epoch 00009: val_f1 improved from 0.80425 to 0.80480, saving model to weights_base.best.h5\n",
      "Epoch 10/15\n",
      "184098/184098 [==============================] - 1449s 8ms/step - loss: 0.4418 - f1: 0.8719 - val_loss: 0.7459 - val_f1: 0.8031\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.80480\n",
      "Epoch 11/15\n",
      "184098/184098 [==============================] - 1451s 8ms/step - loss: 0.4299 - f1: 0.8760 - val_loss: 0.7465 - val_f1: 0.8076\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.80480 to 0.80763, saving model to weights_base.best.h5\n",
      "Epoch 12/15\n",
      "184098/184098 [==============================] - 1452s 8ms/step - loss: 0.4254 - f1: 0.8775 - val_loss: 0.7453 - val_f1: 0.8070\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.80763\n",
      "Epoch 13/15\n",
      "184098/184098 [==============================] - 1455s 8ms/step - loss: 0.4215 - f1: 0.8788 - val_loss: 0.7469 - val_f1: 0.8065\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.80763\n",
      "now is 4 fold\n",
      "Train on 184098 samples, validate on 10228 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184098/184098 [==============================] - 1452s 8ms/step - loss: 0.8764 - f1: 0.7291 - val_loss: 0.8492 - val_f1: 0.7706\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.77063, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184098/184098 [==============================] - 1457s 8ms/step - loss: 0.6472 - f1: 0.8085 - val_loss: 0.7919 - val_f1: 0.7861\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.77063 to 0.78610, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184098/184098 [==============================] - 1457s 8ms/step - loss: 0.5878 - f1: 0.8270 - val_loss: 0.7812 - val_f1: 0.7899\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.78610 to 0.78992, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184098/184098 [==============================] - 1458s 8ms/step - loss: 0.5551 - f1: 0.8372 - val_loss: 0.7897 - val_f1: 0.7935\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.78992 to 0.79346, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184098/184098 [==============================] - 1463s 8ms/step - loss: 0.5299 - f1: 0.8454 - val_loss: 0.7584 - val_f1: 0.7950\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.79346 to 0.79503, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184098/184098 [==============================] - 1464s 8ms/step - loss: 0.5038 - f1: 0.8534 - val_loss: 0.7604 - val_f1: 0.7996\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.79503 to 0.79961, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184098/184098 [==============================] - 1466s 8ms/step - loss: 0.4873 - f1: 0.8588 - val_loss: 0.7458 - val_f1: 0.8029\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.79961 to 0.80286, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "184098/184098 [==============================] - 1467s 8ms/step - loss: 0.4696 - f1: 0.8637 - val_loss: 0.7468 - val_f1: 0.8029\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.80286 to 0.80292, saving model to weights_base.best.h5\n",
      "Epoch 9/15\n",
      "184098/184098 [==============================] - 1464s 8ms/step - loss: 0.4538 - f1: 0.8696 - val_loss: 0.7455 - val_f1: 0.8052\n",
      "\n",
      "Epoch 00009: val_f1 improved from 0.80292 to 0.80516, saving model to weights_base.best.h5\n",
      "Epoch 10/15\n",
      "184098/184098 [==============================] - 1471s 8ms/step - loss: 0.4387 - f1: 0.8743 - val_loss: 0.7539 - val_f1: 0.8037\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.80516\n",
      "Epoch 11/15\n",
      "184098/184098 [==============================] - 1475s 8ms/step - loss: 0.4281 - f1: 0.8767 - val_loss: 0.7502 - val_f1: 0.8049\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.80516\n",
      "now is 5 fold\n",
      "Train on 184098 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "184098/184098 [==============================] - 1471s 8ms/step - loss: 0.8803 - f1: 0.7272 - val_loss: 0.8426 - val_f1: 0.7781\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.77809, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184098/184098 [==============================] - 1474s 8ms/step - loss: 0.6494 - f1: 0.8080 - val_loss: 0.7958 - val_f1: 0.7893\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.77809 to 0.78931, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184098/184098 [==============================] - 1476s 8ms/step - loss: 0.5895 - f1: 0.8255 - val_loss: 0.7618 - val_f1: 0.7966\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.78931 to 0.79655, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184098/184098 [==============================] - 1476s 8ms/step - loss: 0.5535 - f1: 0.8368 - val_loss: 0.7730 - val_f1: 0.7991\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.79655 to 0.79908, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184098/184098 [==============================] - 1477s 8ms/step - loss: 0.5283 - f1: 0.8454 - val_loss: 0.7476 - val_f1: 0.8008\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.79908 to 0.80079, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184098/184098 [==============================] - 1475s 8ms/step - loss: 0.5057 - f1: 0.8529 - val_loss: 0.7538 - val_f1: 0.8060\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.80079 to 0.80602, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184098/184098 [==============================] - 1474s 8ms/step - loss: 0.4868 - f1: 0.8581 - val_loss: 0.7429 - val_f1: 0.8080\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.80602 to 0.80800, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "184098/184098 [==============================] - 1473s 8ms/step - loss: 0.4692 - f1: 0.8637 - val_loss: 0.7360 - val_f1: 0.8096\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.80800 to 0.80962, saving model to weights_base.best.h5\n",
      "Epoch 9/15\n",
      "184098/184098 [==============================] - 1444s 8ms/step - loss: 0.4526 - f1: 0.8693 - val_loss: 0.7448 - val_f1: 0.8092\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.80962\n",
      "Epoch 10/15\n",
      "184098/184098 [==============================] - 1409s 8ms/step - loss: 0.4395 - f1: 0.8733 - val_loss: 0.7379 - val_f1: 0.8120\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.80962 to 0.81204, saving model to weights_base.best.h5\n",
      "Epoch 11/15\n",
      "184098/184098 [==============================] - 1394s 8ms/step - loss: 0.4291 - f1: 0.8768 - val_loss: 0.7338 - val_f1: 0.8114\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.81204\n",
      "Epoch 12/15\n",
      "184098/184098 [==============================] - 1390s 8ms/step - loss: 0.4235 - f1: 0.8780 - val_loss: 0.7402 - val_f1: 0.8118\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.81204\n",
      "now is 6 fold\n",
      "Train on 184098 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "184098/184098 [==============================] - 1402s 8ms/step - loss: 0.8741 - f1: 0.7297 - val_loss: 0.8811 - val_f1: 0.7673\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.76733, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184098/184098 [==============================] - 1419s 8ms/step - loss: 0.6492 - f1: 0.8088 - val_loss: 0.8212 - val_f1: 0.7826\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.76733 to 0.78259, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184098/184098 [==============================] - 1417s 8ms/step - loss: 0.5892 - f1: 0.8265 - val_loss: 0.7869 - val_f1: 0.7891\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.78259 to 0.78913, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184098/184098 [==============================] - 1410s 8ms/step - loss: 0.5571 - f1: 0.8364 - val_loss: 0.7844 - val_f1: 0.7941\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.78913 to 0.79410, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184098/184098 [==============================] - 1410s 8ms/step - loss: 0.5300 - f1: 0.8450 - val_loss: 0.7668 - val_f1: 0.7976\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.79410 to 0.79757, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184098/184098 [==============================] - 1412s 8ms/step - loss: 0.5079 - f1: 0.8524 - val_loss: 0.7519 - val_f1: 0.8008\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.79757 to 0.80077, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184098/184098 [==============================] - 1423s 8ms/step - loss: 0.4872 - f1: 0.8582 - val_loss: 0.7525 - val_f1: 0.8006\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.80077\n",
      "Epoch 8/15\n",
      "184098/184098 [==============================] - 1425s 8ms/step - loss: 0.4708 - f1: 0.8637 - val_loss: 0.7539 - val_f1: 0.8017\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.80077 to 0.80172, saving model to weights_base.best.h5\n",
      "Epoch 9/15\n",
      "184098/184098 [==============================] - 1423s 8ms/step - loss: 0.4542 - f1: 0.8690 - val_loss: 0.7533 - val_f1: 0.8028\n",
      "\n",
      "Epoch 00009: val_f1 improved from 0.80172 to 0.80285, saving model to weights_base.best.h5\n",
      "Epoch 10/15\n",
      "184098/184098 [==============================] - 1424s 8ms/step - loss: 0.4419 - f1: 0.8725 - val_loss: 0.7556 - val_f1: 0.8041\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.80285 to 0.80413, saving model to weights_base.best.h5\n",
      "Epoch 11/15\n",
      "184098/184098 [==============================] - 1425s 8ms/step - loss: 0.4298 - f1: 0.8771 - val_loss: 0.7530 - val_f1: 0.8029\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.80413\n",
      "Epoch 12/15\n",
      "184098/184098 [==============================] - 1427s 8ms/step - loss: 0.4241 - f1: 0.8784 - val_loss: 0.7525 - val_f1: 0.8045\n",
      "\n",
      "Epoch 00012: val_f1 improved from 0.80413 to 0.80450, saving model to weights_base.best.h5\n",
      "Epoch 13/15\n",
      "184098/184098 [==============================] - 1426s 8ms/step - loss: 0.4205 - f1: 0.8794 - val_loss: 0.7555 - val_f1: 0.8037\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.80450\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184098/184098 [==============================] - 1424s 8ms/step - loss: 0.4168 - f1: 0.8797 - val_loss: 0.7521 - val_f1: 0.8046\n",
      "\n",
      "Epoch 00014: val_f1 improved from 0.80450 to 0.80458, saving model to weights_base.best.h5\n",
      "Epoch 15/15\n",
      "184098/184098 [==============================] - 1424s 8ms/step - loss: 0.4132 - f1: 0.8818 - val_loss: 0.7538 - val_f1: 0.8057\n",
      "\n",
      "Epoch 00015: val_f1 improved from 0.80458 to 0.80567, saving model to weights_base.best.h5\n",
      "now is 7 fold\n",
      "Train on 184098 samples, validate on 10228 samples\n",
      "Epoch 1/15\n",
      "184098/184098 [==============================] - 1419s 8ms/step - loss: 0.8721 - f1: 0.7293 - val_loss: 0.8688 - val_f1: 0.7665\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.76646, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184098/184098 [==============================] - 1421s 8ms/step - loss: 0.6455 - f1: 0.8088 - val_loss: 0.8262 - val_f1: 0.7830\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.76646 to 0.78295, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184098/184098 [==============================] - 1420s 8ms/step - loss: 0.5885 - f1: 0.8257 - val_loss: 0.8008 - val_f1: 0.7906\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.78295 to 0.79059, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184098/184098 [==============================] - 1422s 8ms/step - loss: 0.5530 - f1: 0.8378 - val_loss: 0.8016 - val_f1: 0.7935\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.79059 to 0.79354, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184098/184098 [==============================] - 1426s 8ms/step - loss: 0.5265 - f1: 0.8453 - val_loss: 0.7732 - val_f1: 0.7983\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.79354 to 0.79828, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184098/184098 [==============================] - 1424s 8ms/step - loss: 0.5049 - f1: 0.8524 - val_loss: 0.7745 - val_f1: 0.7993\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.79828 to 0.79927, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184098/184098 [==============================] - 1421s 8ms/step - loss: 0.4861 - f1: 0.8586 - val_loss: 0.7686 - val_f1: 0.7997\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.79927 to 0.79968, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "184098/184098 [==============================] - 1423s 8ms/step - loss: 0.4671 - f1: 0.8644 - val_loss: 0.7732 - val_f1: 0.7998\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.79968 to 0.79980, saving model to weights_base.best.h5\n",
      "Epoch 9/15\n",
      "184098/184098 [==============================] - 1425s 8ms/step - loss: 0.4518 - f1: 0.8694 - val_loss: 0.7630 - val_f1: 0.8023\n",
      "\n",
      "Epoch 00009: val_f1 improved from 0.79980 to 0.80233, saving model to weights_base.best.h5\n",
      "Epoch 10/15\n",
      "184098/184098 [==============================] - 1425s 8ms/step - loss: 0.4390 - f1: 0.8736 - val_loss: 0.7687 - val_f1: 0.8035\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.80233 to 0.80346, saving model to weights_base.best.h5\n",
      "Epoch 11/15\n",
      "184098/184098 [==============================] - 1420s 8ms/step - loss: 0.4277 - f1: 0.8770 - val_loss: 0.7725 - val_f1: 0.8065\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.80346 to 0.80646, saving model to weights_base.best.h5\n",
      "Epoch 12/15\n",
      "184098/184098 [==============================] - 1418s 8ms/step - loss: 0.4214 - f1: 0.8785 - val_loss: 0.7717 - val_f1: 0.8048\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.80646\n",
      "Epoch 13/15\n",
      "184098/184098 [==============================] - 1431s 8ms/step - loss: 0.4180 - f1: 0.8799 - val_loss: 0.7734 - val_f1: 0.8056\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.80646\n",
      "now is 8 fold\n",
      "Train on 184100 samples, validate on 10227 samples\n",
      "Epoch 1/15\n",
      "184100/184100 [==============================] - 1437s 8ms/step - loss: 0.8719 - f1: 0.7294 - val_loss: 0.8697 - val_f1: 0.7710\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.77096, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184100/184100 [==============================] - 1446s 8ms/step - loss: 0.6443 - f1: 0.8097 - val_loss: 0.8126 - val_f1: 0.7804\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.77096 to 0.78039, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184100/184100 [==============================] - 1450s 8ms/step - loss: 0.5873 - f1: 0.8270 - val_loss: 0.7869 - val_f1: 0.7888\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.78039 to 0.78877, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184100/184100 [==============================] - 1453s 8ms/step - loss: 0.5530 - f1: 0.8379 - val_loss: 0.7918 - val_f1: 0.7922\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.78877 to 0.79220, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184100/184100 [==============================] - 1456s 8ms/step - loss: 0.5263 - f1: 0.8460 - val_loss: 0.7696 - val_f1: 0.7963\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.79220 to 0.79630, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184100/184100 [==============================] - 1458s 8ms/step - loss: 0.5028 - f1: 0.8535 - val_loss: 0.7751 - val_f1: 0.7968\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.79630 to 0.79680, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184100/184100 [==============================] - 1460s 8ms/step - loss: 0.4844 - f1: 0.8590 - val_loss: 0.7640 - val_f1: 0.8021\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.79680 to 0.80209, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "184100/184100 [==============================] - 1462s 8ms/step - loss: 0.4672 - f1: 0.8649 - val_loss: 0.7692 - val_f1: 0.8016\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.80209\n",
      "Epoch 9/15\n",
      "184100/184100 [==============================] - 1463s 8ms/step - loss: 0.4517 - f1: 0.8693 - val_loss: 0.7702 - val_f1: 0.8012\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.80209\n",
      "now is 9 fold\n",
      "Train on 184100 samples, validate on 10227 samples\n",
      "Epoch 1/15\n",
      "184100/184100 [==============================] - 1410s 8ms/step - loss: 0.8757 - f1: 0.7284 - val_loss: 0.9027 - val_f1: 0.7613\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.76129, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184100/184100 [==============================] - 1404s 8ms/step - loss: 0.6460 - f1: 0.8087 - val_loss: 0.8675 - val_f1: 0.7765\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.76129 to 0.77653, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184100/184100 [==============================] - 1412s 8ms/step - loss: 0.5888 - f1: 0.8261 - val_loss: 0.8521 - val_f1: 0.7817\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.77653 to 0.78173, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184100/184100 [==============================] - 1444s 8ms/step - loss: 0.5508 - f1: 0.8384 - val_loss: 0.8392 - val_f1: 0.7865\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.78173 to 0.78646, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184100/184100 [==============================] - 1458s 8ms/step - loss: 0.5252 - f1: 0.8461 - val_loss: 0.8142 - val_f1: 0.7873\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.78646 to 0.78734, saving model to weights_base.best.h5\n",
      "Epoch 6/15\n",
      "184100/184100 [==============================] - 1468s 8ms/step - loss: 0.5033 - f1: 0.8533 - val_loss: 0.8132 - val_f1: 0.7957\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.78734 to 0.79566, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184100/184100 [==============================] - 1474s 8ms/step - loss: 0.4837 - f1: 0.8595 - val_loss: 0.8115 - val_f1: 0.7965\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.79566 to 0.79645, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "184100/184100 [==============================] - 1478s 8ms/step - loss: 0.4661 - f1: 0.8647 - val_loss: 0.8035 - val_f1: 0.7979\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.79645 to 0.79785, saving model to weights_base.best.h5\n",
      "Epoch 9/15\n",
      "184100/184100 [==============================] - 1474s 8ms/step - loss: 0.4516 - f1: 0.8689 - val_loss: 0.8136 - val_f1: 0.7975\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.79785\n",
      "Epoch 10/15\n",
      "184100/184100 [==============================] - 1422s 8ms/step - loss: 0.4355 - f1: 0.8744 - val_loss: 0.8120 - val_f1: 0.7992\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.79785 to 0.79918, saving model to weights_base.best.h5\n",
      "Epoch 11/15\n",
      "184100/184100 [==============================] - 1412s 8ms/step - loss: 0.4263 - f1: 0.8771 - val_loss: 0.8030 - val_f1: 0.7995\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.79918 to 0.79948, saving model to weights_base.best.h5\n",
      "Epoch 12/15\n",
      "184100/184100 [==============================] - 1414s 8ms/step - loss: 0.4213 - f1: 0.8790 - val_loss: 0.8095 - val_f1: 0.7995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_f1 improved from 0.79948 to 0.79952, saving model to weights_base.best.h5\n",
      "Epoch 13/15\n",
      "184100/184100 [==============================] - 1447s 8ms/step - loss: 0.4175 - f1: 0.8800 - val_loss: 0.8067 - val_f1: 0.8003\n",
      "\n",
      "Epoch 00013: val_f1 improved from 0.79952 to 0.80027, saving model to weights_base.best.h5\n",
      "Epoch 14/15\n",
      "184100/184100 [==============================] - 1459s 8ms/step - loss: 0.4131 - f1: 0.8807 - val_loss: 0.8063 - val_f1: 0.8008\n",
      "\n",
      "Epoch 00014: val_f1 improved from 0.80027 to 0.80079, saving model to weights_base.best.h5\n",
      "Epoch 15/15\n",
      "184100/184100 [==============================] - 1424s 8ms/step - loss: 0.4105 - f1: 0.8816 - val_loss: 0.8104 - val_f1: 0.8011\n",
      "\n",
      "Epoch 00015: val_f1 improved from 0.80079 to 0.80114, saving model to weights_base.best.h5\n",
      "now is 10 fold\n",
      "Train on 184100 samples, validate on 10227 samples\n",
      "Epoch 1/15\n",
      "184100/184100 [==============================] - 1392s 8ms/step - loss: 0.8787 - f1: 0.7272 - val_loss: 0.8733 - val_f1: 0.7669\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.76693, saving model to weights_base.best.h5\n",
      "Epoch 2/15\n",
      "184100/184100 [==============================] - 1397s 8ms/step - loss: 0.6465 - f1: 0.8086 - val_loss: 0.8347 - val_f1: 0.7778\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.76693 to 0.77778, saving model to weights_base.best.h5\n",
      "Epoch 3/15\n",
      "184100/184100 [==============================] - 1401s 8ms/step - loss: 0.5869 - f1: 0.8275 - val_loss: 0.8323 - val_f1: 0.7808\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.77778 to 0.78085, saving model to weights_base.best.h5\n",
      "Epoch 4/15\n",
      "184100/184100 [==============================] - 1443s 8ms/step - loss: 0.5520 - f1: 0.8383 - val_loss: 0.8004 - val_f1: 0.7894\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.78085 to 0.78935, saving model to weights_base.best.h5\n",
      "Epoch 5/15\n",
      "184100/184100 [==============================] - 1454s 8ms/step - loss: 0.5248 - f1: 0.8472 - val_loss: 0.8046 - val_f1: 0.7887\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.78935\n",
      "Epoch 6/15\n",
      "184100/184100 [==============================] - 1459s 8ms/step - loss: 0.5036 - f1: 0.8529 - val_loss: 0.7835 - val_f1: 0.7958\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.78935 to 0.79582, saving model to weights_base.best.h5\n",
      "Epoch 7/15\n",
      "184100/184100 [==============================] - 1461s 8ms/step - loss: 0.4837 - f1: 0.8595 - val_loss: 0.7798 - val_f1: 0.7961\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.79582 to 0.79613, saving model to weights_base.best.h5\n",
      "Epoch 8/15\n",
      "184100/184100 [==============================] - 1463s 8ms/step - loss: 0.4654 - f1: 0.8649 - val_loss: 0.7768 - val_f1: 0.7977\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.79613 to 0.79773, saving model to weights_base.best.h5\n",
      "Epoch 9/15\n",
      "184100/184100 [==============================] - 1466s 8ms/step - loss: 0.4509 - f1: 0.8699 - val_loss: 0.7822 - val_f1: 0.7962\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.79773\n",
      "Epoch 10/15\n",
      "184100/184100 [==============================] - 1468s 8ms/step - loss: 0.4368 - f1: 0.8740 - val_loss: 0.7762 - val_f1: 0.8001\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.79773 to 0.80009, saving model to weights_base.best.h5\n",
      "Epoch 11/15\n",
      "184100/184100 [==============================] - 1469s 8ms/step - loss: 0.4237 - f1: 0.8776 - val_loss: 0.7781 - val_f1: 0.7993\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.80009\n",
      "Epoch 12/15\n",
      "184100/184100 [==============================] - 1469s 8ms/step - loss: 0.4198 - f1: 0.8791 - val_loss: 0.7809 - val_f1: 0.8000\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.80009\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../feature/rnn_pseudo1.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is 0.785668326486411\n"
     ]
    }
   ],
   "source": [
    "val_result = np.argmax(train_pred,axis=1)\n",
    "print ('score is {}'.format(f1_score(Y, val_result, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob=pd.DataFrame(test_pred)\n",
    "test_prob.columns=[\"class_prob_%s\"%i for i in range(1,test_pred.shape[1]+1)]\n",
    "test_prob[\"id\"]=list(test_id[\"id\"])\n",
    "test_prob.to_csv('../pro/prob_pseudo_rnn.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102277, 1)\n",
      "(102277, 1)\n"
     ]
    }
   ],
   "source": [
    "preds=np.argmax(test_pred,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"class\"]\n",
    "test_pred[\"class\"]=(test_pred[\"class\"]+1).astype(int)\n",
    "print(test_pred.shape)\n",
    "print(test_id.shape)\n",
    "test_pred[\"id\"]=list(test_id[\"id\"])\n",
    "test_pred[[\"id\",\"class\"]].to_csv('../output/sub_rnn_pseudo.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
